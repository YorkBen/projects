{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "from sklearn import metrics\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")  # 忽略版本问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = r'Z:\\模型算法组\\训练数据\\zhangkuo\\05基因预测\\Deep处理\\TCGA-colon-counts.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ENSG00000000003.13</th>\n",
       "      <th>ENSG00000000005.5</th>\n",
       "      <th>ENSG00000000419.11</th>\n",
       "      <th>ENSG00000000457.12</th>\n",
       "      <th>ENSG00000000460.15</th>\n",
       "      <th>ENSG00000000938.11</th>\n",
       "      <th>ENSG00000000971.14</th>\n",
       "      <th>ENSG00000001036.12</th>\n",
       "      <th>ENSG00000001084.9</th>\n",
       "      <th>ENSG00000001167.13</th>\n",
       "      <th>...</th>\n",
       "      <th>ENSGR0000264819.4</th>\n",
       "      <th>ENSGR0000265658.4</th>\n",
       "      <th>ENSGR0000270726.4</th>\n",
       "      <th>ENSGR0000275287.3</th>\n",
       "      <th>ENSGR0000276543.3</th>\n",
       "      <th>ENSGR0000277120.3</th>\n",
       "      <th>ENSGR0000280767.1</th>\n",
       "      <th>ENSGR0000281849.1</th>\n",
       "      <th>label1</th>\n",
       "      <th>label2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11.356452</td>\n",
       "      <td>3.584963</td>\n",
       "      <td>10.390169</td>\n",
       "      <td>7.741467</td>\n",
       "      <td>8.375039</td>\n",
       "      <td>7.807355</td>\n",
       "      <td>9.661778</td>\n",
       "      <td>10.403012</td>\n",
       "      <td>10.255029</td>\n",
       "      <td>10.239599</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.925554</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9.250298</td>\n",
       "      <td>7.982994</td>\n",
       "      <td>7.562242</td>\n",
       "      <td>8.810572</td>\n",
       "      <td>9.111136</td>\n",
       "      <td>11.319108</td>\n",
       "      <td>9.396605</td>\n",
       "      <td>8.816984</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11.932953</td>\n",
       "      <td>2.700440</td>\n",
       "      <td>10.715533</td>\n",
       "      <td>9.846274</td>\n",
       "      <td>9.103288</td>\n",
       "      <td>10.921097</td>\n",
       "      <td>11.590821</td>\n",
       "      <td>11.171490</td>\n",
       "      <td>10.966866</td>\n",
       "      <td>10.898223</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13.225810</td>\n",
       "      <td>6.523562</td>\n",
       "      <td>10.862637</td>\n",
       "      <td>9.366322</td>\n",
       "      <td>8.209453</td>\n",
       "      <td>9.063395</td>\n",
       "      <td>10.900867</td>\n",
       "      <td>11.983350</td>\n",
       "      <td>11.151651</td>\n",
       "      <td>10.778077</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10.645658</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.255029</td>\n",
       "      <td>7.539159</td>\n",
       "      <td>7.870365</td>\n",
       "      <td>8.577429</td>\n",
       "      <td>8.228819</td>\n",
       "      <td>11.848232</td>\n",
       "      <td>9.893302</td>\n",
       "      <td>9.366322</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 60485 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ENSG00000000003.13  ENSG00000000005.5  ENSG00000000419.11  \\\n",
       "0           11.356452           3.584963           10.390169   \n",
       "1           10.925554           1.000000            9.250298   \n",
       "2           11.932953           2.700440           10.715533   \n",
       "3           13.225810           6.523562           10.862637   \n",
       "4           10.645658           0.000000            9.255029   \n",
       "\n",
       "   ENSG00000000457.12  ENSG00000000460.15  ENSG00000000938.11  \\\n",
       "0            7.741467            8.375039            7.807355   \n",
       "1            7.982994            7.562242            8.810572   \n",
       "2            9.846274            9.103288           10.921097   \n",
       "3            9.366322            8.209453            9.063395   \n",
       "4            7.539159            7.870365            8.577429   \n",
       "\n",
       "   ENSG00000000971.14  ENSG00000001036.12  ENSG00000001084.9  \\\n",
       "0            9.661778           10.403012          10.255029   \n",
       "1            9.111136           11.319108           9.396605   \n",
       "2           11.590821           11.171490          10.966866   \n",
       "3           10.900867           11.983350          11.151651   \n",
       "4            8.228819           11.848232           9.893302   \n",
       "\n",
       "   ENSG00000001167.13  ...  ENSGR0000264819.4  ENSGR0000265658.4  \\\n",
       "0           10.239599  ...                0.0                0.0   \n",
       "1            8.816984  ...                0.0                0.0   \n",
       "2           10.898223  ...                0.0                0.0   \n",
       "3           10.778077  ...                0.0                0.0   \n",
       "4            9.366322  ...                0.0                0.0   \n",
       "\n",
       "   ENSGR0000270726.4  ENSGR0000275287.3  ENSGR0000276543.3  ENSGR0000277120.3  \\\n",
       "0                0.0                0.0                0.0                0.0   \n",
       "1                0.0                0.0                0.0                0.0   \n",
       "2                0.0                0.0                0.0                0.0   \n",
       "3                0.0                0.0                0.0                0.0   \n",
       "4                0.0                0.0                0.0                0.0   \n",
       "\n",
       "   ENSGR0000280767.1  ENSGR0000281849.1  label1  label2  \n",
       "0                0.0                0.0       0       0  \n",
       "1                0.0                0.0       0       0  \n",
       "2                0.0                0.0       0       0  \n",
       "3                0.0                0.0       0       0  \n",
       "4                0.0                0.0       0       0  \n",
       "\n",
       "[5 rows x 60485 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(csv_path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_features = df.iloc[:,0:-2].values\n",
    "train_labels = df.iloc[:,-2].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# newdf = df.loc[df['label2']!=-1]\n",
    "# newdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# train_features = newdf.iloc[:,0:-2].values\n",
    "# train_labels = newdf.iloc[:,-1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((412, 60483), (412,))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features.shape,train_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x,test_x,train_y,test_y = train_test_split(train_features,train_labels,test_size=0.3,random_state=2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_classifiers = ['NB', 'GNB','KNN', 'LR', 'RF', 'DT', 'SVM', 'GBDT']  \n",
    "test_classifiers = ['GNB','KNN', 'LR', 'RF', 'DT', 'SVM', 'GBDT']\n",
    "# test_classifiers = ['SVM'] \n",
    "\n",
    "classifiers = {'NB':naive_bayes_classifier,  \n",
    "               'GNB':gaussian_classifier,\n",
    "              'KNN':knn_classifier,  \n",
    "               'LR':logistic_regression_classifier,  \n",
    "               'RF':random_forest_classifier,  \n",
    "               'DT':decision_tree_classifier,  \n",
    "              'SVM':svm_classifier,  \n",
    "            'SVMCV':svm_cross_validation,  \n",
    "             'GBDT':gradient_boosting_classifier  \n",
    "}  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_file = None  \n",
    "model_save = {} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "cases = ['是否癌']\n",
    "result_path = r\"Z:\\模型算法组\\训练数据\\zhangkuo\\05基因预测\\Deep处理\\result\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "是否癌,best_acc, LR,85.48387096774194\n",
      "是否癌,best_sens, SVM,93.90243902439023\n",
      "是否癌,best_spec, LR,73.80952380952381\n"
     ]
    }
   ],
   "source": [
    "features_choose_list = []\n",
    "\n",
    "NB_acc_list = []\n",
    "NB_sens_list = []\n",
    "NB_spec_list = []\n",
    "\n",
    "GNB_acc_list = []\n",
    "GNB_sens_list = []\n",
    "GNB_spec_list = []\n",
    "\n",
    "KNN_acc_list = []\n",
    "KNN_sens_list = []\n",
    "KNN_spec_list = []\n",
    "\n",
    "LR_acc_list = []\n",
    "LR_sens_list = []\n",
    "LR_spec_list = []\n",
    "\n",
    "RF_acc_list = []\n",
    "RF_sens_list = []\n",
    "RF_spec_list = []\n",
    "\n",
    "DT_acc_list = []\n",
    "DT_sens_list = []\n",
    "DT_spec_list = []\n",
    "\n",
    "SVM_acc_list = []\n",
    "SVM_sens_list = []\n",
    "SVM_spec_list = []\n",
    "\n",
    "GBDT_acc_list = []\n",
    "GBDT_sens_list = []\n",
    "GBDT_spec_list = []\n",
    "\n",
    "acc_max = 0\n",
    "sens_max = 0\n",
    "spec_max = 0\n",
    "features_max = None\n",
    "methods_max = None\n",
    "\n",
    "acc_dict = { \n",
    "    'NB': NB_acc_list,\n",
    "    'GNB':GNB_acc_list,\n",
    "    'KNN':KNN_acc_list,  \n",
    "    'LR':LR_acc_list,  \n",
    "    'RF':RF_acc_list,  \n",
    "    'DT':DT_acc_list,  \n",
    "    'SVM':SVM_acc_list,   \n",
    "    'GBDT':GBDT_acc_list  \n",
    "}  \n",
    "\n",
    "sens_dict = {\n",
    "    'NB':NB_sens_list,\n",
    "    'GNB':GNB_sens_list,\n",
    "    'KNN':KNN_sens_list,  \n",
    "    'LR':LR_sens_list,  \n",
    "    'RF':RF_sens_list,  \n",
    "    'DT':DT_sens_list,  \n",
    "    'SVM':SVM_sens_list,   \n",
    "    'GBDT':GBDT_sens_list\n",
    "}  \n",
    "\n",
    "spec_dict = {\n",
    "    'NB':NB_sens_list,\n",
    "    'GNB':GNB_spec_list,\n",
    "    'KNN':KNN_spec_list,  \n",
    "    'LR':LR_spec_list,  \n",
    "    'RF':RF_spec_list,  \n",
    "    'DT':DT_spec_list,  \n",
    "    'SVM':SVM_spec_list,   \n",
    "    'GBDT':GBDT_spec_list\n",
    "}  \n",
    "\n",
    "from itertools import combinations,permutations\n",
    "for case_idx in range(1):\n",
    "    \n",
    "    features_choose_list = []\n",
    "    \n",
    "    NB_acc_list = []\n",
    "    NB_sens_list = []\n",
    "    NB_spec_list = []\n",
    "    \n",
    "    GNB_acc_list = []\n",
    "    GNB_sens_list = []\n",
    "    GNB_spec_list = []\n",
    "\n",
    "    KNN_acc_list = []\n",
    "    KNN_sens_list = []\n",
    "    KNN_spec_list = []\n",
    "\n",
    "    LR_acc_list = []\n",
    "    LR_sens_list = []\n",
    "    LR_spec_list = []\n",
    "\n",
    "    RF_acc_list = []\n",
    "    RF_sens_list = []\n",
    "    RF_spec_list = []\n",
    "\n",
    "    DT_acc_list = []\n",
    "    DT_sens_list = []\n",
    "    DT_spec_list = []\n",
    "\n",
    "    SVM_acc_list = []\n",
    "    SVM_sens_list = []\n",
    "    SVM_spec_list = []\n",
    "\n",
    "    GBDT_acc_list = []\n",
    "    GBDT_sens_list = []\n",
    "    GBDT_spec_list = []\n",
    "\n",
    "    acc_max = 0\n",
    "    sens_max = 0\n",
    "    spec_max = 0\n",
    "    features_max = None\n",
    "    methods_max = None\n",
    "\n",
    "    acc_dict = { \n",
    "        'NB':NB_acc_list,\n",
    "        'GNB':GNB_acc_list,\n",
    "        'KNN':KNN_acc_list,  \n",
    "        'LR':LR_acc_list,  \n",
    "        'RF':RF_acc_list,  \n",
    "        'DT':DT_acc_list,  \n",
    "        'SVM':SVM_acc_list,   \n",
    "        'GBDT':GBDT_acc_list  \n",
    "    }  \n",
    "\n",
    "    sens_dict = {\n",
    "        'NB':NB_sens_list,\n",
    "        'GNB':GNB_sens_list,\n",
    "        'KNN':KNN_sens_list,  \n",
    "        'LR':LR_sens_list,  \n",
    "        'RF':RF_sens_list,  \n",
    "        'DT':DT_sens_list,  \n",
    "        'SVM':SVM_sens_list,   \n",
    "        'GBDT':GBDT_sens_list\n",
    "    }  \n",
    "\n",
    "    spec_dict = {\n",
    "        'NB':NB_spec_list,\n",
    "        'GNB':GNB_spec_list,\n",
    "        'KNN':KNN_spec_list,  \n",
    "        'LR':LR_spec_list,  \n",
    "        'RF':RF_spec_list,  \n",
    "        'DT':DT_spec_list,  \n",
    "        'SVM':SVM_spec_list,   \n",
    "        'GBDT':GBDT_spec_list\n",
    "    }  \n",
    "\n",
    "    \n",
    "    acc_max = 0\n",
    "    sens_max = 0\n",
    "    spec_max = 0\n",
    "    features_max = None\n",
    "    methods_max = None\n",
    "\n",
    "\n",
    "    csv_name = '是否癌' + '.csv'\n",
    "\n",
    "\n",
    "    for classifier in test_classifiers:  \n",
    "#         print('******************* %s ********************' % classifier)  \n",
    "        start_time = time.time()  \n",
    "        model = classifiers[classifier](train_x, train_y)  \n",
    "        predict = model.predict(test_x)  \n",
    "\n",
    "        c = confusion_matrix(test_y, predict)\n",
    "#                 print(c)\n",
    "        acc = (c[0][0] + c[1][1]) / (np.sum(c)) * 100\n",
    "        sens = c[1][1] / np.sum(c[1]) * 100\n",
    "        spec = c[0][0] / np.sum(c[0]) * 100\n",
    "#             print(\"准确度:{:.2f}%,灵敏度:{:.2f}%,特异度:{:.2f}%\".format(acc, sens, spec))\n",
    "\n",
    "        if acc > acc_max:\n",
    "#             features_acc_max = choose_features\n",
    "            methods_acc_max = classifier\n",
    "            acc_max = acc\n",
    "        if sens > sens_max:\n",
    "#             features_sens_max = choose_features\n",
    "            methods_sens_max = classifier\n",
    "            sens_max = sens \n",
    "        if spec > spec_max:\n",
    "#             features_spec_max = choose_features\n",
    "            methods_spec_max = classifier\n",
    "            spec_max = spec\n",
    "\n",
    "\n",
    "        acc_dict[classifier].append(\"{:.2f}%\".format(acc))\n",
    "        sens_dict[classifier].append(\"{:.2f}%\".format(sens))\n",
    "        spec_dict[classifier].append(\"{:.2f}%\".format(spec))\n",
    "#         choose_features.append('best_acc_method')\n",
    "#         choose_features.append('best_sens_method')\n",
    "#         choose_features.append('best_spec_method')\n",
    "        \n",
    "#         GNB_acc_list.append(\"{},{},{}\".format(features_acc_max, methods_acc_max, acc_max))\n",
    "#         GNB_acc_list.append(\"{},{},{}\".format(features_sens_max, methods_sens_max, sens_max))\n",
    "#         GNB_acc_list.append(\"{},{},{}\".format(features_spec_max, methods_spec_max, spec_max))\n",
    "        \n",
    "        \n",
    "    print(\"{},best_acc, {},{}\".format(cases[case_idx],  methods_acc_max, acc_max)) \n",
    "    print(\"{},best_sens, {},{}\".format(cases[case_idx],  methods_sens_max, sens_max)) \n",
    "    print(\"{},best_spec, {},{}\".format(cases[case_idx],  methods_spec_max, spec_max)) \n",
    "#     columns = ['features','NB_acc', 'NB_sens', 'NB_spec', 'GNB_acc', 'GNB_sens', 'GNB_spec','KNN_acc', 'KNN_sens', 'KNN_spec', \n",
    "#               'LR_acc', 'LR_sens', 'LR_spec','RF_acc', 'RF_sens', 'RF_spec',\n",
    "#               'DT_acc', 'DT_sens', 'DT_spec','SVM_acc', 'SVM_sens', 'SVM_spec',\n",
    "#               'GBDT_acc', 'GBDT_sens', 'GBDT_spec']\n",
    "    columns = [ 'GNB_acc', 'GNB_sens', 'GNB_spec','KNN_acc', 'KNN_sens', 'KNN_spec', \n",
    "              'LR_acc', 'LR_sens', 'LR_spec','RF_acc', 'RF_sens', 'RF_spec',\n",
    "              'DT_acc', 'DT_sens', 'DT_spec','SVM_acc', 'SVM_sens', 'SVM_spec',\n",
    "              'GBDT_acc', 'GBDT_sens', 'GBDT_spec']\n",
    "    data = [ GNB_acc_list, GNB_sens_list, GNB_spec_list,\n",
    "            KNN_acc_list, KNN_sens_list, KNN_spec_list, LR_acc_list, LR_sens_list, LR_spec_list,\n",
    "            RF_acc_list, RF_sens_list, RF_spec_list, DT_acc_list, DT_sens_list, DT_spec_list,\n",
    "            SVM_acc_list, SVM_sens_list, SVM_spec_list, GBDT_acc_list, GBDT_sens_list, GBDT_spec_list]\n",
    "#     data = [features_choose_list, NB_acc_list, NB_sens_list, NB_spec_list,GNB_acc_list, GNB_sens_list, GNB_spec_list,\n",
    "#             KNN_acc_list, KNN_sens_list, KNN_spec_list, LR_acc_list, LR_sens_list, LR_spec_list,\n",
    "#             RF_acc_list, RF_sens_list, RF_spec_list, DT_acc_list, DT_sens_list, DT_spec_list,\n",
    "#             SVM_acc_list, SVM_sens_list, SVM_spec_list, GBDT_acc_list, GBDT_sens_list, GBDT_spec_list]\n",
    "#     columns = ['features', 'SVM_acc', 'SVM_sens', 'SVM_spec']\n",
    "#     data = [features_choose_list,SVM_acc_list, SVM_sens_list, SVM_spec_list]\n",
    "#     csv_path = os.path.join(result_path, csv_name)\n",
    "#     save_csv(columns, data, csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(85.48387096774194, 91.46341463414635, 73.80952380952381)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model = decision_tree_classifier(train_x, train_y)  \n",
    "model = logistic_regression_classifier(train_x, train_y)  \n",
    "predict = model.predict(test_x)  \n",
    "\n",
    "c = confusion_matrix(test_y, predict)\n",
    "#                 print(c)\n",
    "acc = (c[0][0] + c[1][1]) / (np.sum(c)) * 100\n",
    "sens = c[1][1] / np.sum(c[1]) * 100\n",
    "spec = c[0][0] / np.sum(c[0]) * 100\n",
    "\n",
    "acc, sens, spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60483"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_labels = df.columns.values.tolist()[:-2]\n",
    "len(feat_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60483"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importance = model.coef_[0]\n",
    "len(importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_importances = pd.DataFrame({\n",
    "    \"TCGA\": pd.Series(feat_labels),\n",
    "    \"importance\": pd.Series(importance)\n",
    "})\n",
    "feat_importances.to_csv(r'Z:\\模型算法组\\训练数据\\zhangkuo\\05基因预测\\Deep处理\\TCGA-colon-importance.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "nlargest() missing 1 required positional argument: 'columns'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-7ba98b1d92ab>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mfeat_importances\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlargest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkind\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'barh'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtitle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'Feature Importance'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: nlargest() missing 1 required positional argument: 'columns'"
     ]
    }
   ],
   "source": [
    "feat_importances.nlargest(20).plot(kind='barh',title = 'Feature Importance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00087555, 0.00115727, 0.00332808, ..., 0.        , 0.        ,\n",
       "       0.        ])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feat_labels = [\"年龄\", \"性别\", \"病灶位置\",\"病灶大小\",\"WL置信度\", \"ME置信度\"]\n",
    "for f in range(train_x.shape[1]):\n",
    "    print(\"%2d) %-*s\\t %f\" % (f + 1, 2, feat_labels[indices[f]], importances[indices[f]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [],
   "source": [
    "confs = model.predict_proba(test_x)\n",
    "confs_pred = []\n",
    "for conf in confs:\n",
    "    confs_pred.append(conf[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confs_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.33573374473471335"
      ]
     },
     "execution_count": 456,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "fpr,tpr,thresholds = roc_curve(test_y,confs_pred)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "roc_auc\n",
    "optimal_idx = np.argmax(tpr - fpr)\n",
    "optimal_threshold = thresholds[optimal_idx]\n",
    "optimal_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "灵敏度:91.43%,特异度:90.77%\n"
     ]
    }
   ],
   "source": [
    "thr = 0.35\n",
    "case00 = 0\n",
    "case01 = 0\n",
    "case10 = 0\n",
    "case11 = 0\n",
    "for index, true_label in enumerate(test_y):\n",
    "#     print(index, true_label)\n",
    "    pred_label = 1 if confs_pred[index] >= thr else 0\n",
    "    if true_label == 1:\n",
    "        if pred_label == 1:\n",
    "            case11 += 1\n",
    "        else:\n",
    "            case10+= 1\n",
    "    else:\n",
    "        if pred_label == 1:\n",
    "            case01 += 1\n",
    "        else:\n",
    "            case00 += 1\n",
    "\n",
    "            \n",
    "sens = case11 / (case10 + case11) * 100  # 灵敏度\n",
    "spec = case00 / (case00 + case01) * 100  # 特异度\n",
    "\n",
    "print(f\"灵敏度:{sens:.2f}%,特异度:{spec:.2f}%\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80.64516129032258, 86.58536585365853, 69.04761904761905)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "# iris = load_iris()\n",
    "clf = AdaBoostClassifier(n_estimators=500)\n",
    "clf.fit(train_x, train_y)\n",
    "clf.score(test_x, test_y) \n",
    "predict = clf.predict(test_x)  \n",
    "c = confusion_matrix(test_y, predict)\n",
    "#                 print(c)\n",
    "acc = (c[0][0] + c[1][1]) / (np.sum(c)) * 100\n",
    "sens = c[1][1] / np.sum(c[1]) * 100\n",
    "spec = c[0][0] / np.sum(c[0]) * 100\n",
    "\n",
    "acc, sens, spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [],
   "source": [
    "confs = clf.predict_proba(test_x)\n",
    "confs_pred = []\n",
    "for conf in confs:\n",
    "    confs_pred.append(conf[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confs_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5082104783031258"
      ]
     },
     "execution_count": 433,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "fpr,tpr,thresholds = roc_curve(test_y,confs_pred)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "roc_auc\n",
    "optimal_idx = np.argmax(tpr - fpr)\n",
    "optimal_threshold = thresholds[optimal_idx]\n",
    "optimal_threshold\n",
    "# optimal_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "灵敏度:88.57%,特异度:76.92%\n"
     ]
    }
   ],
   "source": [
    "thr = 0.48\n",
    "case00 = 0\n",
    "case01 = 0\n",
    "case10 = 0\n",
    "case11 = 0\n",
    "for index, true_label in enumerate(test_y):\n",
    "#     print(index, true_label)\n",
    "    pred_label = 1 if confs_pred[index] >= thr else 0\n",
    "    if true_label == 1:\n",
    "        if pred_label == 1:\n",
    "            case11 += 1\n",
    "        else:\n",
    "            case10+= 1\n",
    "    else:\n",
    "        if pred_label == 1:\n",
    "            case01 += 1\n",
    "        else:\n",
    "            case00 += 1\n",
    "\n",
    "            \n",
    "sens = case11 / (case10 + case11) * 100  # 灵敏度\n",
    "spec = case00 / (case00 + case01) * 100  # 特异度\n",
    "\n",
    "print(f\"灵敏度:{sens:.2f}%,特异度:{spec:.2f}%\") \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1) 病灶大小\t 0.394375\n",
      " 2) 年龄\t 0.194804\n",
      " 3) 病灶位置\t 0.139988\n",
      " 4) ME置信度\t 0.136719\n",
      " 5) 性别\t 0.104818\n",
      " 6) WL置信度\t 0.029297\n"
     ]
    }
   ],
   "source": [
    "feat_labels = [\"年龄\", \"性别\", \"病灶位置\",\"病灶大小\",\"WL置信度\", \"ME置信度\"]\n",
    "for f in range(train_x.shape[1]):\n",
    "    print(\"%2d) %-*s\\t %f\" % (f + 1, 2, feat_labels[indices[f]], importances[indices[f]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(89.0, 85.71428571428571, 90.76923076923077)"
      ]
     },
     "execution_count": 411,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "# iris = load_iris()\n",
    "clf = AdaBoostClassifier(n_estimators=500,learning_rate=0.1)\n",
    "clf.fit(train_x, train_y)\n",
    "clf.score(test_x, test_y) \n",
    "predict = clf.predict(test_x)  \n",
    "c = confusion_matrix(test_y, predict)\n",
    "#                 print(c)\n",
    "acc = (c[0][0] + c[1][1]) / (np.sum(c)) * 100\n",
    "sens = c[1][1] / np.sum(c[1]) * 100\n",
    "spec = c[0][0] / np.sum(c[0]) * 100\n",
    "\n",
    "acc, sens, spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to file in the current working directory\n",
    "pkl_filename = r\"Z:\\模型算法组\\训练数据\\zhangkuo\\03 多模态拟合\\白光+放大+基线拟合\\第一届测试集\\AdaBoostClassifier_model.pkl\"\n",
    "with open(pkl_filename, 'wb') as file:\n",
    "    pickle.dump(model, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.77"
      ]
     },
     "execution_count": 396,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.linear_model import RidgeCV, LassoCV\n",
    "from sklearn.svm import SVR\n",
    "estimators = [('ridge', RidgeCV()), ('lasso', LassoCV(random_state=42)), ('svr', SVR(C=1, gamma=1e-6))]\n",
    "reg = StackingRegressor(estimators=estimators,final_estimator=GradientBoostingRegressor(random_state=42))\n",
    "clf.fit(train_x, train_y)\n",
    "clf.score(test_x, test_y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(77.0, 65.71428571428571, 83.07692307692308)"
      ]
     },
     "execution_count": 394,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import make_hastie_10_2\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# X, y = make_hastie_10_2(random_state=0)\n",
    "# X_train, X_test = X[:2000], X[2000:]\n",
    "# y_train, y_test = y[:2000], y[2000:]\n",
    "\n",
    "clf = GradientBoostingClassifier(n_estimators=500, learning_rate=0.1, max_depth=10, random_state=0)\n",
    "clf.fit(train_x, train_y)\n",
    "clf.score(test_x, test_y)  # 0.913...\n",
    "predict = clf.predict(test_x)  \n",
    "c = confusion_matrix(test_y, predict)\n",
    "#                 print(c)\n",
    "acc = (c[0][0] + c[1][1]) / (np.sum(c)) * 100\n",
    "sens = c[1][1] / np.sum(c[1]) * 100\n",
    "spec = c[0][0] / np.sum(c[0]) * 100\n",
    "\n",
    "acc, sens, spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier,VotingClassifier,BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier \n",
    "# tree.DecisionTreeClassifier()  \n",
    "bag_clf=BaggingClassifier(RandomForestClassifier(),\n",
    "                          n_estimators=500,\n",
    "                          max_samples=90,\n",
    "                          bootstrap=True,\n",
    "                          n_jobs=-1,\n",
    "                          oob_score=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(90, 6)"
      ]
     },
     "execution_count": 420,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 422,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(90.0, 77.14285714285715, 96.92307692307692)"
      ]
     },
     "execution_count": 390,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_clf.fit(train_x, train_y)  \n",
    "predict = bag_clf.predict(test_x)  \n",
    "\n",
    "c = confusion_matrix(test_y, predict)\n",
    "#                 print(c)\n",
    "acc = (c[0][0] + c[1][1]) / (np.sum(c)) * 100\n",
    "sens = c[1][1] / np.sum(c[1]) * 100\n",
    "spec = c[0][0] / np.sum(c[0]) * 100\n",
    "\n",
    "acc, sens, spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save to file in the current working directory\n",
    "pkl_filename = r\"Z:\\模型算法组\\训练数据\\zhangkuo\\03 多模态拟合\\白光+放大+基线拟合\\第一届测试集\\GNB_model.pkl\"\n",
    "with open(pkl_filename, 'wb') as file:\n",
    "    pickle.dump(model, file)\n",
    "\n",
    "# Load from file\n",
    "# with open(pkl_filename, 'rb') as file:\n",
    "#     pickle_model = pickle.load(file)\n",
    "    \n",
    "# # Calculate the accuracy score and predict target values\n",
    "# score = pickle_model.score(Xtest, Ytest)\n",
    "# print(\"Test score: {0:.2f} %\".format(100 * score))\n",
    "# Ypredict = pickle_model.predict(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.002  0.002  0.     0.114  0.046  0.102]\n",
      "['年龄', '性别', '病灶位置', '病灶大小', 'WL置信度', 'ME置信度']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "# from sklearn.naive_bayes import GaussianNB\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(inputs, target, test_size=0.2)\n",
    "\n",
    "# gaussian_nb = GaussianNB()\n",
    "# gaussian_nb.fit(X_train, y_train)\n",
    "\n",
    "imps = permutation_importance(model, test_x, test_y)\n",
    "print(imps.importances_mean)\n",
    "print([\"年龄\", \"性别\", \"病灶位置\",\"病灶大小\",\"WL置信度\", \"ME置信度\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'GaussianNB' object has no attribute 'feature_importances_'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-347-a5c9379ead1f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# model.feature_importances_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mimportances\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_importances_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimportances\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'GaussianNB' object has no attribute 'feature_importances_'"
     ]
    }
   ],
   "source": [
    "# model.feature_importances_\n",
    "importances = model.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_labels = [\"年龄\", \"性别\", \"病灶位置\",\"病灶大小\",\"WL置信度\", \"ME置信度\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1) 病灶大小\t 0.394375\n",
      " 2) 年龄\t 0.194804\n",
      " 3) 病灶位置\t 0.139988\n",
      " 4) ME置信度\t 0.136719\n",
      " 5) 性别\t 0.104818\n",
      " 6) WL置信度\t 0.029297\n"
     ]
    }
   ],
   "source": [
    "for f in range(train_x.shape[1]):\n",
    "    print(\"%2d) %-*s\\t %f\" % (f + 1, 2, feat_labels[indices[f]], importances[indices[f]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

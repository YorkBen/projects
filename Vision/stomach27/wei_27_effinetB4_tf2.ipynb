{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 体内外数据训练\n",
    "## 导入相应的包\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-02T12:15:43.860364Z",
     "start_time": "2022-06-02T12:15:43.617013Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-03 15:20:59,655 - INFO - __init__.py - init_logger - 58 - ini  logger file D:\\projects\\Vision\\Lib\\logs\\kernel-67e15a2c-3fcd-4bb7-adb0-647aa7c999a0.log\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use temp dir:d:\\znyx\\temp\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# plt.rcParams['font.sans-serif']=['SimHei'] #用来正常显示中文标签\n",
    "# plt.rcParams['axes.unicode_minus']=False #用来正常显示负号\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from tensorflow.keras.applications import EfficientNetB4\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout\n",
    "from tensorflow.keras.models import Model, model_from_json\n",
    "# 添加路径，以能正常导入mbsh、trainer\n",
    "sys.path.insert(0, r\"..\\Lib\\trainer\")\n",
    "\n",
    "\n",
    "\n",
    "from mbsh import create_app\n",
    "from mbsh.core.models import SmallModel\n",
    "from mbsh.core.images import read_to_pd,TrainArgs\n",
    "from mbsh.core.plot import Plot\n",
    "\n",
    "from trainer import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-30T10:17:38.817064Z",
     "start_time": "2022-05-30T10:17:37.974169Z"
    }
   },
   "outputs": [],
   "source": [
    "# model_id = 100 ResNet 模型 loss: 0.2732 - acc: 0.9096 - val_loss: 0.4109 - val_acc: 0.8740\n",
    "\n",
    "import tensorflow as tf\n",
    "# config = tf.ConfigProto()\n",
    "# config.gpu_options.per_process_gpu_memory_fraction = 0.7  # 程序最多只能占用指定gpu70%的显存\n",
    "# config.gpu_options.allow_growth = True      #程序按需申请内存\n",
    "# sess = tf.Session(config = config)\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "for device in physical_devices:\n",
    "  tf.config.experimental.set_memory_growth(device, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  路径常量设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-30T10:17:41.225956Z",
     "start_time": "2022-05-30T10:17:41.218947Z"
    }
   },
   "outputs": [],
   "source": [
    "# 样本数据根目录\n",
    "# 根目录下res中按分类标签存放各类型图像\n",
    "# cache目录存放训练的结果模型\n",
    "# \n",
    "root_path = r\"\\\\192.168.0.154\\数据\\口咽部\\大部位27分类\"\n",
    "\n",
    "# 是否使用平衡数据做训练\n",
    "use_increase = False\n",
    "\n",
    "# 训练采用的模型名称\n",
    "model_name = 'efficientnet'\n",
    "\n",
    "# 模型保存id\n",
    "model_id = 28015\n",
    "\n",
    "\n",
    "IMG_SIZE = 224\n",
    "img_size = (IMG_SIZE, IMG_SIZE)\n",
    "\n",
    "# img_size = (360,360)\n",
    "# 训练迭代次数\n",
    "epochs = 3\n",
    "\n",
    "\n",
    "# 训练每批次样本数\n",
    "batch_size = 16 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-30T10:17:42.033766Z",
     "start_time": "2022-05-30T10:17:42.019808Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-03 15:21:00,478 - INFO - __init__.py - create_app - 71 - db url =sqlite:///D:\\projects\\Vision\\Lib\\trainer\\../data\\production.db\n"
     ]
    }
   ],
   "source": [
    "# app创建\n",
    "app = create_app(os.getenv('FLASK_CONFIG') or 'default')\n",
    "app.app_context().push()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-30T10:17:42.623759Z",
     "start_time": "2022-05-30T10:17:42.602779Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0-无法判断', '1-食管', '2-贲门', '3-胃窦#大弯', '4-胃窦#后壁', '5-胃窦#前壁', '6-胃窦#小弯', '7-十二指肠球部', '8-十二指肠降部', '9-正镜胃体下部#大弯', '10-正镜胃体下部#后壁', '11-正镜胃体下部#前壁', '12-正镜胃体下部#小弯', '13-正镜胃体中上部#大弯', '14-正镜胃体中上部#后壁', '15-正镜胃体中上部#前壁', '16-正镜胃体中上部#小弯', '17-倒镜胃底#大弯', '18-倒镜胃底#后壁', '19-倒镜胃底#前壁', '20-倒镜胃底#小弯', '21-倒镜胃体中上部#后壁', '22-倒镜胃体中上部#前壁', '23-倒镜胃体中上部#小弯', '24-倒镜胃角#后壁', '25-倒镜胃角#前壁', '26-倒镜胃角#小弯', '27-胃#咽部']\n"
     ]
    }
   ],
   "source": [
    "cache_path = root_path + '/cache'\n",
    "res_path = root_path + '/res'\n",
    "\n",
    "# res数据分割后的训练和验证集目录\n",
    "res_train_path = root_path + '/res_train'\n",
    "res_test_path = root_path + '/res_val'\n",
    "\n",
    "# 数据平衡后的训练和验证集目录\n",
    "train_path = root_path + '/train'\n",
    "test_path = root_path + '/val'\n",
    "\n",
    "# 图片预测错误存放目录\n",
    "pred_err_path = root_path + '/pred_err'\n",
    "\n",
    "\n",
    "# trainer创建\n",
    "\n",
    "# 从数据库获取model信息，如果数据库还未建立相应的记录可查询一个已存在的，再自行修改sm的desc_list\n",
    "# sm = SmallModel.query.get(sm_name)\n",
    "\n",
    "# 生成 trainer\n",
    "sm_name = '胃27模型'\n",
    "sm_desc_list =  ['无法判断', '食管', '贲门', '胃窦#大弯', '胃窦#后壁', '胃窦#前壁', '胃窦#小弯', '十二指肠球部', \n",
    "                 '十二指肠降部', '正镜胃体下部#大弯', '正镜胃体下部#后壁', '正镜胃体下部#前壁', '正镜胃体下部#小弯',\n",
    "                 '正镜胃体中上部#大弯', '正镜胃体中上部#后壁', '正镜胃体中上部#前壁', '正镜胃体中上部#小弯','倒镜胃底#大弯', \n",
    "                 '倒镜胃底#后壁', '倒镜胃底#前壁', '倒镜胃底#小弯','倒镜胃体中上部#后壁', '倒镜胃体中上部#前壁', \n",
    "                 '倒镜胃体中上部#小弯','倒镜胃角#后壁', '倒镜胃角#前壁', '倒镜胃角#小弯', '胃#咽部']\n",
    "sm = SmallModel(sm_name)\n",
    "\n",
    "\n",
    "sm.desc_list = sm_desc_list\n",
    "trainer = Trainer(sm)\n",
    "trainer.img_size = img_size\n",
    "trainer.target_fold = root_path\n",
    "\n",
    "# 类型个数\n",
    "types_num = len(trainer.desc_list)\n",
    "\n",
    "desc_list = [str(x) + '-' + trainer.desc_list[x] for x in range(0,types_num)]\n",
    "print(desc_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 开始训练\n",
    "### 数据增强"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-30T10:17:48.167129Z",
     "start_time": "2022-05-30T10:17:44.098930Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 70552 images belonging to 28 classes.\n",
      "Found 7395 images belonging to 28 classes.\n"
     ]
    }
   ],
   "source": [
    "def img_rgb2_bgr(img):\n",
    "    return img[: , : , : : -1]\n",
    "\n",
    "# 图片生成器\n",
    "# https://keras-cn.readthedocs.io/en/latest/preprocessing/image/#imagedatagenerator\n",
    "train_datagen =  ImageDataGenerator(       \n",
    "        width_shift_range=0.1,\n",
    "        height_shift_range=0.1,\n",
    "        shear_range=0.1,\n",
    "        rotation_range=10,\n",
    "        zoom_range=0.1,\n",
    "        fill_mode='constant',\n",
    "#         vertical_flip=True,\n",
    "#         horizontal_flip=True,\n",
    "        preprocessing_function = img_rgb2_bgr,\n",
    "    )\n",
    "\n",
    "test_datagen =  ImageDataGenerator(\n",
    "        fill_mode='constant',\n",
    "        preprocessing_function = img_rgb2_bgr,\n",
    "    )\n",
    "\n",
    "# 训练数据与测试数据\n",
    "cls_mode = 'binary' if types_num == 2 else 'categorical'\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        res_train_path,\n",
    "        target_size = img_size,\n",
    "        batch_size = batch_size,\n",
    "        class_mode = cls_mode)\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "        res_test_path,\n",
    "        target_size = img_size,\n",
    "        batch_size = batch_size,\n",
    "        class_mode = cls_mode)\n",
    "\n",
    "train_samples = train_generator.samples\n",
    "valid_samples = test_generator.samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练\n",
    "#### 先全部冻结训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-30T10:17:54.038112Z",
     "start_time": "2022-05-30T10:17:49.378580Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-03 15:21:10,746 - DEBUG - attrs.py - __getitem__ - 77 - Creating converter from 3 to 5\n"
     ]
    }
   ],
   "source": [
    "IMG_SHAPE = (IMG_SIZE, IMG_SIZE, 3)\n",
    "# Create the base model Resnet50\n",
    "base_model = EfficientNetB4(input_shape=IMG_SHAPE,include_top=False, weights='imagenet')\n",
    "# base_model = MobileNetV2(input_shape=IMG_SHAPE,include_top=False, weights='imagenet')\n",
    "# 冻结所有层\n",
    "base_model.trainable = False\n",
    "# base_model.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-30T10:17:54.162511Z",
     "start_time": "2022-05-30T10:17:54.039049Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\gradient_descent.py:108: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import SGD\n",
    "setting = (1, 'sigmoid', 'binary_crossentropy') if types_num == 2 else (types_num, 'softmax', 'categorical_crossentropy')\n",
    "# Add a classification head\n",
    "x = GlobalAveragePooling2D()(base_model.output)\n",
    "x = Dropout(0.5)(x)\n",
    "prediction = Dense(setting[0], activation=setting[1], name='dense')(x)\n",
    "\n",
    "model = Model(base_model.input, prediction)\n",
    "\n",
    "    \n",
    "base_learning_rate = 0.0001\n",
    "sgd = SGD(lr=base_learning_rate, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(optimizer=sgd, loss=setting[2], metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-30T10:17:54.177475Z",
     "start_time": "2022-05-30T10:17:54.163510Z"
    }
   },
   "outputs": [],
   "source": [
    "def fit_gen(model, model_id, epochs=5):\n",
    "    class_mode = 'binary' if types_num == 2 else 'categorical'\n",
    "    print(\"train from imgs model_id=%s ,class_mode=%s\" % (model_id, class_mode))\n",
    "                \n",
    "    # 创建cache目录\n",
    "    if not os.path.exists(cache_path):\n",
    "        os.mkdir(cache_path)\n",
    "    \n",
    "    weight_path = cache_path + '/weights' + str(model_id) + '.hdf5'\n",
    "    \n",
    "    # EarlyStoppingy原型：\n",
    "    # EarlyStopping(monitor='val_loss', min_delta=0, patience=0, verbose=0, mode='auto', baseline=None, restore_best_weights=False)\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=10)\n",
    "    \n",
    "    # ModelCheckpoint原型：\n",
    "    # ModelCheckpoint(filepath, monitor='val_loss', verbose=0, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "    check_point = ModelCheckpoint(weight_path, monitor='val_loss', save_best_only=True, save_weights_only=True)\n",
    "    \n",
    "    # callbacks设置\n",
    "    callbacks = [early_stop, check_point]\n",
    "    \n",
    "    \n",
    "    history = model.fit(\n",
    "        train_generator,\n",
    "        epochs = epochs,\n",
    "        steps_per_epoch = train_samples // batch_size,\n",
    "        validation_data = test_generator,\n",
    "        validation_steps = valid_samples // batch_size,\n",
    "        callbacks = callbacks)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-30T13:56:55.965939Z",
     "start_time": "2022-05-30T10:17:54.178469Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train from imgs model_id=28015 ,class_mode=categorical\n",
      "Epoch 1/20\n",
      "4409/4409 [==============================] - ETA: 0s - loss: 2.8534 - accuracy: 0.2097"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-03 15:53:58,422 - DEBUG - attrs.py - create - 203 - Creating converter from 5 to 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4409/4409 [==============================] - 1958s 441ms/step - loss: 2.8534 - accuracy: 0.2097 - val_loss: 2.2697 - val_accuracy: 0.4138\n",
      "Epoch 2/20\n",
      "4409/4409 [==============================] - 1574s 357ms/step - loss: 2.2453 - accuracy: 0.3711 - val_loss: 1.8894 - val_accuracy: 0.4922\n",
      "Epoch 3/20\n",
      "4409/4409 [==============================] - 1922s 436ms/step - loss: 1.9893 - accuracy: 0.4246 - val_loss: 1.7026 - val_accuracy: 0.5290\n",
      "Epoch 4/20\n",
      "4409/4409 [==============================] - 1593s 361ms/step - loss: 1.8450 - accuracy: 0.4560 - val_loss: 1.5886 - val_accuracy: 0.5491\n",
      "Epoch 5/20\n",
      "4409/4409 [==============================] - 1798s 408ms/step - loss: 1.7522 - accuracy: 0.4778 - val_loss: 1.5117 - val_accuracy: 0.5644\n",
      "Epoch 6/20\n",
      "4409/4409 [==============================] - 1227s 278ms/step - loss: 1.6864 - accuracy: 0.4913 - val_loss: 1.4531 - val_accuracy: 0.5736\n",
      "Epoch 7/20\n",
      "4409/4409 [==============================] - 802s 182ms/step - loss: 1.6317 - accuracy: 0.5035 - val_loss: 1.4084 - val_accuracy: 0.5854\n",
      "Epoch 8/20\n",
      "4409/4409 [==============================] - 800s 181ms/step - loss: 1.5900 - accuracy: 0.5161 - val_loss: 1.3724 - val_accuracy: 0.5946\n",
      "Epoch 9/20\n",
      "4409/4409 [==============================] - 1605s 364ms/step - loss: 1.5551 - accuracy: 0.5245 - val_loss: 1.3414 - val_accuracy: 0.6023\n",
      "Epoch 10/20\n",
      "4409/4409 [==============================] - 1242s 282ms/step - loss: 1.5278 - accuracy: 0.5311 - val_loss: 1.3173 - val_accuracy: 0.6057\n",
      "Epoch 11/20\n",
      "4409/4409 [==============================] - 798s 181ms/step - loss: 1.5012 - accuracy: 0.5388 - val_loss: 1.2946 - val_accuracy: 0.6109\n",
      "Epoch 12/20\n",
      "4409/4409 [==============================] - 945s 214ms/step - loss: 1.4793 - accuracy: 0.5421 - val_loss: 1.2738 - val_accuracy: 0.6172\n",
      "Epoch 13/20\n",
      "4409/4409 [==============================] - 2209s 501ms/step - loss: 1.4594 - accuracy: 0.5501 - val_loss: 1.2579 - val_accuracy: 0.6201\n",
      "Epoch 14/20\n",
      "4409/4409 [==============================] - 825s 187ms/step - loss: 1.4479 - accuracy: 0.5500 - val_loss: 1.2420 - val_accuracy: 0.6245\n",
      "Epoch 15/20\n",
      "4409/4409 [==============================] - 1014s 230ms/step - loss: 1.4287 - accuracy: 0.5568 - val_loss: 1.2285 - val_accuracy: 0.6292\n",
      "Epoch 16/20\n",
      "4409/4409 [==============================] - 1503s 341ms/step - loss: 1.4167 - accuracy: 0.5602 - val_loss: 1.2167 - val_accuracy: 0.6304\n",
      "Epoch 17/20\n",
      "4409/4409 [==============================] - 868s 197ms/step - loss: 1.3975 - accuracy: 0.5673 - val_loss: 1.2050 - val_accuracy: 0.6335\n",
      "Epoch 18/20\n",
      "4409/4409 [==============================] - 1024s 232ms/step - loss: 1.3875 - accuracy: 0.5672 - val_loss: 1.1935 - val_accuracy: 0.6358\n",
      "Epoch 19/20\n",
      "4409/4409 [==============================] - 817s 185ms/step - loss: 1.3767 - accuracy: 0.5703 - val_loss: 1.1846 - val_accuracy: 0.6391\n",
      "Epoch 20/20\n",
      "4409/4409 [==============================] - 810s 184ms/step - loss: 1.3658 - accuracy: 0.5730 - val_loss: 1.1764 - val_accuracy: 0.6393\n"
     ]
    }
   ],
   "source": [
    "history = fit_gen(model, model_id, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-30T13:56:56.189369Z",
     "start_time": "2022-05-30T13:56:55.966937Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of layers in the base model:  475\n"
     ]
    }
   ],
   "source": [
    "base_model.trainable = True\n",
    "# Let's take a look to see how many layers are in the base model\n",
    "print(\"Number of layers in the base model: \", len(base_model.layers))\n",
    "# 在 fine_tune_at 之后放开训练,根据情况修改 \n",
    "fine_tune_at = 120\n",
    "\n",
    "# Freeze all the layers before the `fine_tune_at` layer\n",
    "for layer in base_model.layers[:fine_tune_at]:\n",
    "    layer.trainable =  False\n",
    "    \n",
    "x = GlobalAveragePooling2D()(base_model.output)\n",
    "x = Dropout(0.5)(x)\n",
    "prediction = Dense(setting[0], activation=setting[1], name='dense')(x)\n",
    "\n",
    "model = Model(base_model.input, prediction)\n",
    "\n",
    "    \n",
    "# base_learning_rate = 0.0001\n",
    "sgd = SGD(lr=base_learning_rate/10, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(optimizer=sgd, loss=setting[2], metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-30T17:34:15.410477Z",
     "start_time": "2022-05-30T13:56:56.190371Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train from imgs model_id=28015 ,class_mode=categorical\n",
      "Epoch 1/100\n",
      "4409/4409 [==============================] - 854s 191ms/step - loss: 3.3739 - accuracy: 0.0463 - val_loss: 3.2741 - val_accuracy: 0.0680\n",
      "Epoch 2/100\n",
      "4409/4409 [==============================] - 836s 190ms/step - loss: 3.2861 - accuracy: 0.0684 - val_loss: 3.1865 - val_accuracy: 0.1126\n",
      "Epoch 3/100\n",
      "4409/4409 [==============================] - 881s 200ms/step - loss: 3.2005 - accuracy: 0.0970 - val_loss: 3.0957 - val_accuracy: 0.1680\n",
      "Epoch 4/100\n",
      "4409/4409 [==============================] - 881s 200ms/step - loss: 3.1098 - accuracy: 0.1325 - val_loss: 2.9961 - val_accuracy: 0.2294\n",
      "Epoch 5/100\n",
      "4409/4409 [==============================] - 876s 198ms/step - loss: 3.0107 - accuracy: 0.1694 - val_loss: 2.8855 - val_accuracy: 0.2777\n",
      "Epoch 6/100\n",
      "4409/4409 [==============================] - 873s 198ms/step - loss: 2.9012 - accuracy: 0.2087 - val_loss: 2.7621 - val_accuracy: 0.3233\n",
      "Epoch 7/100\n",
      "4409/4409 [==============================] - 839s 190ms/step - loss: 2.7766 - accuracy: 0.2513 - val_loss: 2.6262 - val_accuracy: 0.3654\n",
      "Epoch 8/100\n",
      "4409/4409 [==============================] - 838s 190ms/step - loss: 2.6469 - accuracy: 0.2891 - val_loss: 2.4864 - val_accuracy: 0.4072\n",
      "Epoch 9/100\n",
      "4409/4409 [==============================] - 855s 194ms/step - loss: 2.5146 - accuracy: 0.3247 - val_loss: 2.3384 - val_accuracy: 0.4406\n",
      "Epoch 10/100\n",
      "4409/4409 [==============================] - 838s 190ms/step - loss: 2.3796 - accuracy: 0.3593 - val_loss: 2.1980 - val_accuracy: 0.4685\n",
      "Epoch 11/100\n",
      "4409/4409 [==============================] - 877s 199ms/step - loss: 2.2452 - accuracy: 0.3911 - val_loss: 2.0650 - val_accuracy: 0.4922\n",
      "Epoch 12/100\n",
      "4409/4409 [==============================] - 880s 200ms/step - loss: 2.1186 - accuracy: 0.4173 - val_loss: 1.9309 - val_accuracy: 0.5156\n",
      "Epoch 13/100\n",
      "4409/4409 [==============================] - 875s 198ms/step - loss: 2.0017 - accuracy: 0.4425 - val_loss: 1.8178 - val_accuracy: 0.5365\n",
      "Epoch 14/100\n",
      "4409/4409 [==============================] - 875s 198ms/step - loss: 1.8960 - accuracy: 0.4647 - val_loss: 1.7002 - val_accuracy: 0.5599\n",
      "Epoch 15/100\n",
      "4409/4409 [==============================] - 850s 193ms/step - loss: 1.8066 - accuracy: 0.4805 - val_loss: 1.6013 - val_accuracy: 0.5772\n",
      "Epoch 16/100\n",
      "4409/4409 [==============================] - 838s 190ms/step - loss: 1.7216 - accuracy: 0.4987 - val_loss: 1.5253 - val_accuracy: 0.5900\n",
      "Epoch 17/100\n",
      "4409/4409 [==============================] - 858s 195ms/step - loss: 1.6451 - accuracy: 0.5150 - val_loss: 1.4442 - val_accuracy: 0.6092\n",
      "Epoch 18/100\n",
      "4409/4409 [==============================] - 837s 190ms/step - loss: 1.5695 - accuracy: 0.5335 - val_loss: 1.3675 - val_accuracy: 0.6258\n",
      "Epoch 19/100\n",
      "4409/4409 [==============================] - 859s 195ms/step - loss: 1.5167 - accuracy: 0.5438 - val_loss: 1.3015 - val_accuracy: 0.6412\n",
      "Epoch 20/100\n",
      "4409/4409 [==============================] - 882s 200ms/step - loss: 1.4561 - accuracy: 0.5596 - val_loss: 1.2434 - val_accuracy: 0.6518\n",
      "Epoch 21/100\n",
      "4409/4409 [==============================] - 876s 199ms/step - loss: 1.4036 - accuracy: 0.5726 - val_loss: 1.1882 - val_accuracy: 0.6637\n",
      "Epoch 22/100\n",
      "4409/4409 [==============================] - 875s 198ms/step - loss: 1.3628 - accuracy: 0.5821 - val_loss: 1.1450 - val_accuracy: 0.6736\n",
      "Epoch 23/100\n",
      "4409/4409 [==============================] - 868s 197ms/step - loss: 1.3164 - accuracy: 0.5914 - val_loss: 1.0992 - val_accuracy: 0.6841\n",
      "Epoch 24/100\n",
      "4409/4409 [==============================] - 837s 190ms/step - loss: 1.2784 - accuracy: 0.6043 - val_loss: 1.0593 - val_accuracy: 0.6917\n",
      "Epoch 25/100\n",
      "4409/4409 [==============================] - 864s 196ms/step - loss: 1.2424 - accuracy: 0.6147 - val_loss: 1.0203 - val_accuracy: 0.7027\n",
      "Epoch 26/100\n",
      "4409/4409 [==============================] - 835s 189ms/step - loss: 1.2099 - accuracy: 0.6200 - val_loss: 0.9868 - val_accuracy: 0.7112\n",
      "Epoch 27/100\n",
      "4409/4409 [==============================] - 845s 192ms/step - loss: 1.1808 - accuracy: 0.6302 - val_loss: 0.9576 - val_accuracy: 0.7174\n",
      "Epoch 28/100\n",
      "4409/4409 [==============================] - 878s 199ms/step - loss: 1.1514 - accuracy: 0.6385 - val_loss: 0.9290 - val_accuracy: 0.7246\n",
      "Epoch 29/100\n",
      "4409/4409 [==============================] - 878s 199ms/step - loss: 1.1279 - accuracy: 0.6413 - val_loss: 0.8991 - val_accuracy: 0.7307\n",
      "Epoch 30/100\n",
      "4409/4409 [==============================] - 878s 199ms/step - loss: 1.1019 - accuracy: 0.6512 - val_loss: 0.8756 - val_accuracy: 0.7367\n",
      "Epoch 31/100\n",
      "4409/4409 [==============================] - 879s 199ms/step - loss: 1.0820 - accuracy: 0.6561 - val_loss: 0.8529 - val_accuracy: 0.7408\n",
      "Epoch 32/100\n",
      "4409/4409 [==============================] - 837s 190ms/step - loss: 1.0595 - accuracy: 0.6614 - val_loss: 0.8287 - val_accuracy: 0.7474\n",
      "Epoch 33/100\n",
      "4409/4409 [==============================] - 839s 190ms/step - loss: 1.0373 - accuracy: 0.6691 - val_loss: 0.8126 - val_accuracy: 0.7511\n",
      "Epoch 34/100\n",
      "4409/4409 [==============================] - 837s 190ms/step - loss: 1.0170 - accuracy: 0.6757 - val_loss: 0.7938 - val_accuracy: 0.7550\n",
      "Epoch 35/100\n",
      "4409/4409 [==============================] - 840s 190ms/step - loss: 1.0009 - accuracy: 0.6797 - val_loss: 0.7725 - val_accuracy: 0.7623\n",
      "Epoch 36/100\n",
      "4409/4409 [==============================] - 864s 196ms/step - loss: 0.9820 - accuracy: 0.6852 - val_loss: 0.7568 - val_accuracy: 0.7669\n",
      "Epoch 37/100\n",
      "4409/4409 [==============================] - 877s 199ms/step - loss: 0.9656 - accuracy: 0.6906 - val_loss: 0.7398 - val_accuracy: 0.7692\n",
      "Epoch 38/100\n",
      "4409/4409 [==============================] - 875s 198ms/step - loss: 0.9484 - accuracy: 0.6956 - val_loss: 0.7266 - val_accuracy: 0.7727\n",
      "Epoch 39/100\n",
      "4409/4409 [==============================] - 881s 200ms/step - loss: 0.9329 - accuracy: 0.7004 - val_loss: 0.7090 - val_accuracy: 0.7769\n",
      "Epoch 40/100\n",
      "4409/4409 [==============================] - 852s 193ms/step - loss: 0.9261 - accuracy: 0.7005 - val_loss: 0.6964 - val_accuracy: 0.7804\n",
      "Epoch 41/100\n",
      "4409/4409 [==============================] - 838s 190ms/step - loss: 0.9111 - accuracy: 0.7068 - val_loss: 0.6887 - val_accuracy: 0.7838\n",
      "Epoch 42/100\n",
      "4409/4409 [==============================] - 855s 194ms/step - loss: 0.8968 - accuracy: 0.7106 - val_loss: 0.6711 - val_accuracy: 0.7860\n",
      "Epoch 43/100\n",
      "4409/4409 [==============================] - 845s 192ms/step - loss: 0.8854 - accuracy: 0.7158 - val_loss: 0.6597 - val_accuracy: 0.7894\n",
      "Epoch 44/100\n",
      "4409/4409 [==============================] - 852s 193ms/step - loss: 0.8705 - accuracy: 0.7180 - val_loss: 0.6531 - val_accuracy: 0.7917\n",
      "Epoch 45/100\n",
      "4409/4409 [==============================] - 915s 207ms/step - loss: 0.8619 - accuracy: 0.7199 - val_loss: 0.6426 - val_accuracy: 0.7952\n",
      "Epoch 46/100\n",
      "4409/4409 [==============================] - 874s 198ms/step - loss: 0.8453 - accuracy: 0.7254 - val_loss: 0.6312 - val_accuracy: 0.7968\n",
      "Epoch 47/100\n",
      "4409/4409 [==============================] - 875s 198ms/step - loss: 0.8391 - accuracy: 0.7287 - val_loss: 0.6215 - val_accuracy: 0.7996\n",
      "Epoch 48/100\n",
      "4409/4409 [==============================] - 871s 197ms/step - loss: 0.8338 - accuracy: 0.7285 - val_loss: 0.6127 - val_accuracy: 0.8021\n",
      "Epoch 49/100\n",
      "4409/4409 [==============================] - 839s 190ms/step - loss: 0.8228 - accuracy: 0.7316 - val_loss: 0.6065 - val_accuracy: 0.8032\n",
      "Epoch 50/100\n",
      "4409/4409 [==============================] - 838s 190ms/step - loss: 0.8172 - accuracy: 0.7332 - val_loss: 0.5979 - val_accuracy: 0.8053\n",
      "Epoch 51/100\n",
      "4409/4409 [==============================] - 838s 190ms/step - loss: 0.7966 - accuracy: 0.7389 - val_loss: 0.5887 - val_accuracy: 0.8063\n",
      "Epoch 52/100\n",
      "4409/4409 [==============================] - 840s 191ms/step - loss: 0.7925 - accuracy: 0.7407 - val_loss: 0.5828 - val_accuracy: 0.8099\n",
      "Epoch 53/100\n",
      "4409/4409 [==============================] - 899s 204ms/step - loss: 0.7867 - accuracy: 0.7429 - val_loss: 0.5744 - val_accuracy: 0.8107\n",
      "Epoch 54/100\n",
      "4409/4409 [==============================] - 877s 199ms/step - loss: 0.7785 - accuracy: 0.7441 - val_loss: 0.5703 - val_accuracy: 0.8120\n",
      "Epoch 55/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4409/4409 [==============================] - 871s 198ms/step - loss: 0.7718 - accuracy: 0.7458 - val_loss: 0.5633 - val_accuracy: 0.8147\n",
      "Epoch 56/100\n",
      "4409/4409 [==============================] - 882s 200ms/step - loss: 0.7643 - accuracy: 0.7490 - val_loss: 0.5575 - val_accuracy: 0.8162\n",
      "Epoch 57/100\n",
      "4409/4409 [==============================] - 842s 191ms/step - loss: 0.7635 - accuracy: 0.7509 - val_loss: 0.5524 - val_accuracy: 0.8166\n",
      "Epoch 58/100\n",
      "4409/4409 [==============================] - 839s 190ms/step - loss: 0.7548 - accuracy: 0.7519 - val_loss: 0.5485 - val_accuracy: 0.8189\n",
      "Epoch 59/100\n",
      "4409/4409 [==============================] - 840s 190ms/step - loss: 0.7491 - accuracy: 0.7547 - val_loss: 0.5400 - val_accuracy: 0.8206\n",
      "Epoch 60/100\n",
      "4409/4409 [==============================] - 843s 191ms/step - loss: 0.7421 - accuracy: 0.7549 - val_loss: 0.5387 - val_accuracy: 0.8205\n",
      "Epoch 61/100\n",
      "4409/4409 [==============================] - 866s 196ms/step - loss: 0.7393 - accuracy: 0.7575 - val_loss: 0.5333 - val_accuracy: 0.8222\n",
      "Epoch 62/100\n",
      "4409/4409 [==============================] - 877s 199ms/step - loss: 0.7300 - accuracy: 0.7600 - val_loss: 0.5282 - val_accuracy: 0.8239\n",
      "Epoch 63/100\n",
      "4409/4409 [==============================] - 882s 200ms/step - loss: 0.7196 - accuracy: 0.7633 - val_loss: 0.5230 - val_accuracy: 0.8239\n",
      "Epoch 64/100\n",
      "4409/4409 [==============================] - 877s 199ms/step - loss: 0.7168 - accuracy: 0.7633 - val_loss: 0.5206 - val_accuracy: 0.8252\n",
      "Epoch 65/100\n",
      "4409/4409 [==============================] - 887s 201ms/step - loss: 0.7145 - accuracy: 0.7636 - val_loss: 0.5150 - val_accuracy: 0.8264\n",
      "Epoch 66/100\n",
      "4409/4409 [==============================] - 840s 190ms/step - loss: 0.7078 - accuracy: 0.7653 - val_loss: 0.5139 - val_accuracy: 0.8279\n",
      "Epoch 67/100\n",
      "4409/4409 [==============================] - 841s 191ms/step - loss: 0.7071 - accuracy: 0.7666 - val_loss: 0.5078 - val_accuracy: 0.8293\n",
      "Epoch 68/100\n",
      "4409/4409 [==============================] - 843s 191ms/step - loss: 0.7020 - accuracy: 0.7667 - val_loss: 0.5064 - val_accuracy: 0.8316\n",
      "Epoch 69/100\n",
      "4409/4409 [==============================] - 851s 193ms/step - loss: 0.6933 - accuracy: 0.7694 - val_loss: 0.5006 - val_accuracy: 0.8329\n",
      "Epoch 70/100\n",
      "4409/4409 [==============================] - 876s 199ms/step - loss: 0.6916 - accuracy: 0.7696 - val_loss: 0.5007 - val_accuracy: 0.8316\n",
      "Epoch 71/100\n",
      "4409/4409 [==============================] - 883s 200ms/step - loss: 0.6868 - accuracy: 0.7714 - val_loss: 0.4948 - val_accuracy: 0.8324\n",
      "Epoch 72/100\n",
      "4409/4409 [==============================] - 880s 200ms/step - loss: 0.6797 - accuracy: 0.7763 - val_loss: 0.4902 - val_accuracy: 0.8344\n",
      "Epoch 73/100\n",
      "4409/4409 [==============================] - 868s 197ms/step - loss: 0.6760 - accuracy: 0.7762 - val_loss: 0.4868 - val_accuracy: 0.8354\n",
      "Epoch 74/100\n",
      "4409/4409 [==============================] - 839s 190ms/step - loss: 0.6758 - accuracy: 0.7755 - val_loss: 0.4840 - val_accuracy: 0.8358\n",
      "Epoch 75/100\n",
      "4409/4409 [==============================] - 842s 191ms/step - loss: 0.6712 - accuracy: 0.7762 - val_loss: 0.4851 - val_accuracy: 0.8355\n",
      "Epoch 76/100\n",
      "4409/4409 [==============================] - 838s 190ms/step - loss: 0.6696 - accuracy: 0.7797 - val_loss: 0.4832 - val_accuracy: 0.8364\n",
      "Epoch 77/100\n",
      "4409/4409 [==============================] - 862s 196ms/step - loss: 0.6648 - accuracy: 0.7779 - val_loss: 0.4771 - val_accuracy: 0.8393\n",
      "Epoch 78/100\n",
      "4409/4409 [==============================] - 878s 199ms/step - loss: 0.6599 - accuracy: 0.7807 - val_loss: 0.4743 - val_accuracy: 0.8385\n",
      "Epoch 79/100\n",
      "4409/4409 [==============================] - 878s 199ms/step - loss: 0.6576 - accuracy: 0.7804 - val_loss: 0.4729 - val_accuracy: 0.8381\n",
      "Epoch 80/100\n",
      "4409/4409 [==============================] - 887s 201ms/step - loss: 0.6579 - accuracy: 0.7812 - val_loss: 0.4716 - val_accuracy: 0.8392\n",
      "Epoch 81/100\n",
      "4409/4409 [==============================] - 877s 199ms/step - loss: 0.6497 - accuracy: 0.7849 - val_loss: 0.4680 - val_accuracy: 0.8409\n",
      "Epoch 82/100\n",
      "4409/4409 [==============================] - 846s 192ms/step - loss: 0.6464 - accuracy: 0.7835 - val_loss: 0.4670 - val_accuracy: 0.8413\n",
      "Epoch 83/100\n",
      "4409/4409 [==============================] - 845s 192ms/step - loss: 0.6465 - accuracy: 0.7840 - val_loss: 0.4634 - val_accuracy: 0.8415\n",
      "Epoch 84/100\n",
      "4409/4409 [==============================] - 841s 191ms/step - loss: 0.6449 - accuracy: 0.7838 - val_loss: 0.4609 - val_accuracy: 0.8408\n",
      "Epoch 85/100\n",
      "4409/4409 [==============================] - 837s 190ms/step - loss: 0.6378 - accuracy: 0.7875 - val_loss: 0.4607 - val_accuracy: 0.8431\n",
      "Epoch 86/100\n",
      "4409/4409 [==============================] - 861s 195ms/step - loss: 0.6377 - accuracy: 0.7885 - val_loss: 0.4587 - val_accuracy: 0.8420\n",
      "Epoch 87/100\n",
      "4409/4409 [==============================] - 888s 201ms/step - loss: 0.6322 - accuracy: 0.7894 - val_loss: 0.4574 - val_accuracy: 0.8431\n",
      "Epoch 88/100\n",
      "4409/4409 [==============================] - 883s 200ms/step - loss: 0.6331 - accuracy: 0.7888 - val_loss: 0.4528 - val_accuracy: 0.8432\n",
      "Epoch 89/100\n",
      "4409/4409 [==============================] - 882s 200ms/step - loss: 0.6277 - accuracy: 0.7898 - val_loss: 0.4535 - val_accuracy: 0.8442\n",
      "Epoch 90/100\n",
      "4409/4409 [==============================] - 862s 195ms/step - loss: 0.6258 - accuracy: 0.7903 - val_loss: 0.4504 - val_accuracy: 0.8450\n",
      "Epoch 91/100\n",
      "4409/4409 [==============================] - 838s 190ms/step - loss: 0.6242 - accuracy: 0.7920 - val_loss: 0.4475 - val_accuracy: 0.8454\n",
      "Epoch 92/100\n",
      "4409/4409 [==============================] - 840s 191ms/step - loss: 0.6177 - accuracy: 0.7932 - val_loss: 0.4495 - val_accuracy: 0.8446\n",
      "Epoch 93/100\n",
      "4409/4409 [==============================] - 838s 190ms/step - loss: 0.6167 - accuracy: 0.7942 - val_loss: 0.4480 - val_accuracy: 0.8448\n",
      "Epoch 94/100\n",
      "4409/4409 [==============================] - 846s 192ms/step - loss: 0.6180 - accuracy: 0.7932 - val_loss: 0.4467 - val_accuracy: 0.8456\n",
      "Epoch 95/100\n",
      "4409/4409 [==============================] - 876s 199ms/step - loss: 0.6124 - accuracy: 0.7964 - val_loss: 0.4414 - val_accuracy: 0.8467\n",
      "Epoch 96/100\n",
      "4409/4409 [==============================] - 885s 201ms/step - loss: 0.6098 - accuracy: 0.7955 - val_loss: 0.4404 - val_accuracy: 0.8470\n",
      "Epoch 97/100\n",
      "4409/4409 [==============================] - 878s 199ms/step - loss: 0.6097 - accuracy: 0.7954 - val_loss: 0.4403 - val_accuracy: 0.8470\n",
      "Epoch 98/100\n",
      "4409/4409 [==============================] - 875s 198ms/step - loss: 0.6024 - accuracy: 0.7976 - val_loss: 0.4371 - val_accuracy: 0.8474\n",
      "Epoch 99/100\n",
      "4409/4409 [==============================] - 839s 190ms/step - loss: 0.6078 - accuracy: 0.7954 - val_loss: 0.4372 - val_accuracy: 0.8492\n",
      "Epoch 100/100\n",
      "4409/4409 [==============================] - 842s 191ms/step - loss: 0.6017 - accuracy: 0.7984 - val_loss: 0.4338 - val_accuracy: 0.8501\n"
     ]
    }
   ],
   "source": [
    "history = fit_gen(model, model_id, epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 保存模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-30T17:34:27.303452Z",
     "start_time": "2022-05-30T17:34:15.413470Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-04 22:18:08,017 - INFO - trainer.py - save_model - 565 - save model success ,num=28015\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model(model, model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_id = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 测试集测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-31T00:31:26.927972Z",
     "start_time": "2022-05-31T00:31:17.914967Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-31 08:31:26,919 - INFO - trainer.py - load_model - 580 - load weights 28011\n"
     ]
    }
   ],
   "source": [
    "model = trainer.load_model(model_id)\n",
    "# 加载训练中的最优模型\n",
    "# model.load_weights(trainer.get_weight_file(model_id))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-31T00:38:32.825342Z",
     "start_time": "2022-05-31T00:38:27.516Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_path = root_path + '/res_test'\n",
    "# use_increase = True\n",
    "# 显示预测准确度、混淆矩阵，保持混淆矩阵图到root_path\n",
    "# test_path = res_test_path\n",
    "# acc, y_pred, y_true, file_test,_ = trainer.predict_data([test_path if use_increase else res_test_path], model = model,binary_threshold=0.5)\n",
    "acc, y_pred, y_true, file_test = trainer.predict_data([pred_path], model = model,binary_threshold=0.5)\n",
    "\n",
    "\n",
    "path = Plot.show_matrix(y_pred, y_true, types_num, root_path, fig_size=10)\n",
    "plt.show()\n",
    "\n",
    "print(desc_list)    \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "use_increase = True\n",
    "# 显示预测准确度、混淆矩阵，保持混淆矩阵图到root_path\n",
    "acc, y_pred, y_true, file_test = trainer.predict_data([test_path if use_increase else res_test_path], model = model,binary_threshold=optimal_threshold)\n",
    "path = Plot.show_matrix(y_pred, y_true, types_num, root_path)\n",
    "plt.show()\n",
    "\n",
    "print(desc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "acc: 98.51%\n",
    "0: 98.69%\n",
    "1: 98.35%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8853171155516942\n"
     ]
    }
   ],
   "source": [
    "## 测试\n",
    "\n",
    "import cv2\n",
    "from natsort import natsorted\n",
    "\n",
    "from mbsh.core.images import sort_by_file_name,load_img_data,fetch_all_files\n",
    "imgs_path0 = r'E:\\tmp\\001_0_171315_2020_09_03_05_51_257'\n",
    "\n",
    "nums = 0\n",
    "# 获取该路径下所有图片\n",
    "files = os.listdir(imgs_path0)\n",
    "files = natsorted(files)\n",
    "src_size = 224\n",
    "new_size = 224\n",
    "def read_img(path):\n",
    "    img = cv2.imdecode(np.fromfile(path, dtype=np.uint8), cv2.IMREAD_UNCHANGED) #cv2.imread(path)\n",
    "    img = cv2.resize(img, (src_size, src_size), cv2.INTER_LINEAR)\n",
    "        \n",
    "    return img\n",
    "\n",
    "# self.lesion_similar = self.beta2 * self.lesion_similar + (1 - self.beta2) * similarity\n",
    "pred = 0\n",
    "for file in files:\n",
    "    fpath = os.path.join(imgs_path0, file)\n",
    "    img = read_img(fpath)\n",
    "    \n",
    "    x = np.array(img,dtype=\"float32\")\n",
    "\n",
    "#     xx = np.random.randint(0, src_size-new_size)\n",
    "#     yy = np.random.randint(0, src_size-new_size)\n",
    "    xx=int((src_size-new_size)//2)\n",
    "    yy=int((src_size-new_size)//2)\n",
    "    x = x[xx:xx+new_size, yy:yy+new_size,:]\n",
    "    \n",
    "    if x.shape[2] != 3:\n",
    "        \n",
    "        x = x[:, :, :3]\n",
    "        \n",
    "    predictions = model.predict(np.asarray([x]), verbose=0)\n",
    "    \n",
    "    if predictions[0][0] > 0.5:\n",
    "        nums += 1\n",
    "#     pred = 0.8 * pred + (1 - 0.8) * predictions[0][0]     \n",
    "#     print(file,\": \",predictions[0][0])\n",
    "        \n",
    "print(nums/len(files))\n",
    "# 224 0.8637037037037038\n",
    "#256  0.9807407407407407\n",
    "# 232 0.9348148148148148\n",
    "#     print(file,\": \",predictions[0][0])\n",
    "# 212张\n",
    "# 224  0.37264150943396224\n",
    "# 256 0.7688679245283019\n",
    "# 300 0.9669811320754716"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "    y = np.random.randint(0, h-size)\n",
    "    x = np.random.randint(0, w-size)\n",
    "\n",
    "    image = image[y:y+size, x:x+size, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "from mbsh.core.images import sort_by_file_name,load_img_data,fetch_all_files\n",
    "imgs_path0 = r'E:\\projects\\znyx-trainer\\job_2020\\imgs_in_out\\test\\00'\n",
    "imgs_path1 = r'E:\\projects\\znyx-trainer\\job_2020\\imgs_in_out\\test\\01'\n",
    "binary_threshold = 0.5\n",
    "pred0_0 = 0\n",
    "pred0_1 = 0\n",
    "pred1_0 = 0\n",
    "pred1_1 = 0\n",
    "\n",
    "src_size = 300\n",
    "new_size = 224\n",
    "# 获取该路径下所有图片\n",
    "files_list0=fetch_all_files(imgs_path0)\n",
    "files_list0=sort_by_file_name(files_list0)\n",
    "\n",
    "files_list1=fetch_all_files(imgs_path1)\n",
    "files_list1=sort_by_file_name(files_list1)\n",
    "label0 = [0] * len(files_list0)\n",
    "label1 = [1] * len(files_list1)\n",
    "files_list = files_list0 + files_list1\n",
    "label = label0 + label1\n",
    "x_test = []\n",
    "X = []\n",
    "def read_img(path):\n",
    "    img = cv2.imdecode(np.fromfile(path, dtype=np.uint8), cv2.IMREAD_UNCHANGED) #cv2.imread(path)\n",
    "    img = cv2.resize(img, (src_size, src_size), cv2.INTER_LINEAR)\n",
    "    return img\n",
    "\n",
    "for file in files_list0:\n",
    "    img = read_img(file)\n",
    "    x_test.append(img)\n",
    "\n",
    "for x in x_test:\n",
    "    x = np.array(x,dtype=\"float32\")\n",
    "    \n",
    "    xx = np.random.randint(0, src_size-new_size)\n",
    "    yy = np.random.randint(0, src_size-new_size)\n",
    "\n",
    "    xx=int((src_size-new_size)//2)\n",
    "    yy=int((src_size-new_size)//2)\n",
    "    x = x[xx:xx+new_size, yy:yy+new_size,:]\n",
    "    if x.shape[2] != 3:\n",
    "        x = x[:, :, :3]\n",
    "    predictions = model.predict(np.asarray([x]), verbose=0)[0][0]\n",
    "    y_pred = 1 if predictions > binary_threshold else 0\n",
    "    \n",
    "    if y_pred == 0:\n",
    "        pred0_0 += 1\n",
    "    else:\n",
    "        pred0_1 += 1\n",
    "        \n",
    "x_test = []\n",
    "X = []     \n",
    "for file in files_list1:\n",
    "    img = read_img(file)\n",
    "    x_test.append(img)\n",
    "\n",
    "for x in x_test:\n",
    "    x = np.array(x,dtype=\"float32\")\n",
    "    xx = np.random.randint(0, src_size-new_size)\n",
    "    yy = np.random.randint(0, src_size-new_size)\n",
    "\n",
    "    xx=int((src_size-new_size)//2)\n",
    "    yy=int((src_size-new_size)//2)\n",
    "    x = x[xx:xx+new_size, yy:yy+new_size,:]\n",
    "    if x.shape[2] != 3:\n",
    "        x = x[:, :, :3]\n",
    "    predictions = model.predict(np.asarray([x]), verbose=0)[0][0]\n",
    "    y_pred = 1 if predictions > binary_threshold else 0\n",
    "    \n",
    "    if y_pred == 0:\n",
    "        pred1_0 += 1\n",
    "    else:\n",
    "        pred1_1 += 1   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "acc = (pred0_0 + pred1_1) / (pred0_0 + pred1_1 + pred0_1 + pred1_0) * 100\n",
    "acc0 = (pred0_0) / (pred0_0 +pred0_1) * 100\n",
    "acc1 = (pred1_1) / (pred1_1 + pred1_0) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "准确率:96.62%, 体外准确率:93.78%,体内准确率:99.12%\n"
     ]
    }
   ],
   "source": [
    "print(f\"准确率:{acc:.2f}%, 体外准确率:{acc0:.2f}%,体内准确率:{acc1:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 直接resize224 准确率:98.51%, 体外准确率:98.69%,体内准确率:98.35%\n",
    "# 256 准确率:97.75%, 体外准确率:96.19%,体内准确率:99.12%\n",
    "# 232 准确率:98.63%, 体外准确率:98.34%,体内准确率:98.88%\n",
    "# 准确率:96.62%, 体外准确率:93.78%,体内准确率:99.12%\n",
    "# random crop\n",
    "# 256 准确率:97.71%, 体外准确率:96.32%,体内准确率:98.92%\n",
    "# 232 准确率:98.46%, 体外准确率:98.20%,体内准确率:98.69%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4884/4884 [==============================] - 8s 2ms/sample\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from mbsh.core.images import sort_by_file_name,load_img_data,fetch_all_files\n",
    "imgs_path0 = r'E:\\projects\\znyx-trainer\\job_2020\\imgs_in_out\\test\\00'\n",
    "imgs_path1 = r'E:\\projects\\znyx-trainer\\job_2020\\imgs_in_out\\test\\01'\n",
    "\n",
    "# 获取该路径下所有图片\n",
    "files_list0=fetch_all_files(imgs_path0)\n",
    "files_list0=sort_by_file_name(files_list0)\n",
    "\n",
    "files_list1=fetch_all_files(imgs_path1)\n",
    "files_list1=sort_by_file_name(files_list1)\n",
    "label0 = [0] * len(files_list0)\n",
    "label1 = [1] * len(files_list1)\n",
    "files_list = files_list0 + files_list1\n",
    "label = label0 + label1\n",
    "x_test = []\n",
    "X = []\n",
    "def read_img(path):\n",
    "    img = cv2.imdecode(np.fromfile(path, dtype=np.uint8), cv2.IMREAD_UNCHANGED) #cv2.imread(path)\n",
    "    img = cv2.resize(img, (224, 224), cv2.INTER_LINEAR)\n",
    "    return img\n",
    "\n",
    "for file in files_list:\n",
    "    img = read_img(file)\n",
    "    x_test.append(img)\n",
    "\n",
    "for x in x_test:\n",
    "    x = np.array(x,dtype=\"float32\")\n",
    "    if x.shape[2] != 3:\n",
    "        x = x[:, :, :3]\n",
    "    X.append(x)\n",
    "\n",
    "X= np.asarray(X)\n",
    "\n",
    "predictions_list = model.predict(X, verbose=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred = []\n",
    "for i in predictions_list:\n",
    "#     print(i[0])\n",
    "    pred.append(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fpr,tpr,thresholds = roc_curve(label,pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9981371244779739"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc = auc(fpr, tpr)\n",
    "roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "130"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimal_idx = np.argmax(tpr - fpr)\n",
    "optimal_threshold = thresholds[optimal_idx]\n",
    "optimal_threshold\n",
    "optimal_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.37347806"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimal_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAl4AAAHrCAYAAAAezpPdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nO3deZRdVZn38e+TkZBECBDCPDZDM0Uw0gwBAxpecQCJc9MgorKcQNvufsEWbaDt1pe2aQfENooToDSg4AjGCZmhEyVoUBQZgkggQkjIPD3vH+fEqpRVt27Fursqt76ftWrl3HP3PeepOlTyY+999onMRJIkSa03bKALkCRJGioMXpIkSYUYvCRJkgoxeEmSJBVi8JIkSSrE4CVJklSIwUuSJKkQg5c0hEXEJyLi9QNdR08i4ksRsTgiFkbEQxHxd53e+1BEPBURj0XEjE77p0fEwxExPyLe1eR5vhoRH+v0+oKI+Gin11dHxBn19j/Ux34oIl7xF3xvb4+IBRExOyL27KXtLhHx/Yh4IiJ+EBG79LJ/VERcX+//RUT8TZfjfX7D9yOpLIOXNIRl5nsy839KnjMi9ujjP/rnZuZE4LXAZRGxXURMB84EDgZmAF+MiG0iYgJwJfB64CDgfRGxfxPnOA54cRO1HwacBuxXn/fyiBjZh+9lw3EOBj4IHAacDVzay0c+DlyfmTsC3wG+0Mv+twG/qPefC1ze6dwX1d+DpAFg8JJU2h7AGX39UGbOAR4B9gJOAr6WmU9m5v8C84DjgZOB/83MezJzCfADqlDVo4g4EJgPbB0R2/ZSxoHAwsxckZn3UoWnLfr6vQCvAr6SmX/IzDuB7SJibA/1jQJeCnyu3nUlcFRP++vtA4A76+2fAjvXxzoA2AYoGrYldTB4SUNYPZR3RqfXZ0TEVfXX0xHx9YiIXo7xT/XQ2wMRcWK974KIuKBTm0fqnq5bgW9QBYcFEfG5Hg7b3XkOAXalI3w92unt+VSB7iDggU77/x34Vi+HfjFwM3ArvYQ04BbgiIj4TETslJkzM/O5TjV+IiJO7/WbgV2A+zq9/gOwe4P2I4Gt6+2DgT/2sv+XwJsiYjzwDqoACvCrzHw3sL6JGiW1gMFLUlevBq6mCgJHA8/vqWFEvAR4EzAZOIVqyG9ST+0z8xiqIbo7MnOHzHxbfZyf1UGs89c76o/9v4j4I1UweldmPkXVy7Sq06FXA2OoQsjSTuebn5mPR8Q3uzn+v9fNjqfqFfopvQw3ZuajVL1KewO/iYg3dXn/PZn5lUbHqA0HlnR6vYyOANX1nKuB64FvRsQHgCuAa3raX3/sGqpr9z3gHODi+lg+nFcaYCMGugBJg87szPw2QEQ8AGzVoO2JwJWZuQhYFBF3A8d0065hr1lmHtbd/npS+LlU85fmUQUJgOVsPMQ3ut63pt7e8PkZwIrMPLmH4w8HjgWOpPof0UWN6qxr/QVwQkScBFwdEXdl5gO9fa6LRWwctMbQuBfqDOAtdZ0TgU/0sv8S4P9m5tciYjvgloh4QWau6GOdkvqZPV6Suvpdp+1mekiyy/ZGn4mIEUCPvWDNyMzfA7PomBv2ENXQ4ga7Aw8DD1INQ27wMqDRHYMvAB7OzEn1BP5xEbErsBIY1andKGBFRHx4Qy9XZn4L+AnVEF9fzaYKS9RDuYcBj/fUODNXZuangSeB/87MxxvtB44A7q/b/JGqR22PTahTUj8zeEnqqi/DUTcCp0bE1vXdg38D3EY1jLZr3eYsOvVCUc1D2iUihkfEhLrXqRmfBN5ZB5VvAW+IiEl1r9hfU4WgbwAviYiD6yHPl1LN3+rJ8cDdnV7fXe+7F5geEeMjYgeqkDSXai7ZmyNiTERsTxW65jZZf2c3AqfUPXIfBJ7OzMcafaCe+P9Gqnlrve1/GHhHROxfzznbu65d0gBzqFHSJsvMH0bEFVQTxVcCZ2bmkxFxNXBjRMyiurvu0U6f+WVE/JCqh2cd8FdAr0NgmXlrRCwFpmfmrIj4PNUk8lXAGZ2GO08DrgO2BP4tM+9vcNgXU90NuMHd9b43UQ2jPkA1BHhJZv46Ih4EplD1Cq4CLszM3274cER8ApjT2zyvzFwSEa8B/qM+zqn1548CPpCZL+/mY+cCM+s5br3tfx/V0hI/A56iui7LGtUkqYxwrqUkSVIZ9nhJ6lVELOhm91OZeUjxYiRpM2aPlyRJUiFOrpckSSrE4CVJklTIZjHHa7vttss99thjoMuQJEnq1Zw5c/5Yrw34ZzaL4LXHHnswe/bsgS5DkiSpVxHxaE/vOdQoSZJUiMFLkiSpEIOXJElSIQYvSZKkQgxekiRJhRi8JEmSCjF4SZIkFWLwkiRJKsTgJUmSVIjBS5IkqRCDlyRJUiEGL0mSpEIMXpIkSYUYvCRJkgoxeEmSJBXSkuAVEZMi4tYG74+MiG9HxO0RcWYrapAkSRps+j14RcQE4MvA2AbNzgbmZObRwGsiYnx/1yFJkjTYjGjBMdcBrwe+2aDNNOC8evsWYArwkxbUogG2Zt16Vq1dP9BlNLR89VoWLF7Zb8d7dvkanlm2uk+fSZKVa9azZMUalq1a22k/rF63nlVrqp/jqrXrWL12PbkphSWsW5+sXZ+sXb++2l5Xba/90/YmHVmSNhvvP3F/jt134oCdv9+DV2YuAYiIRs3GAo/X288Ak7o2iIizgLMAdtttt/4tcpBatGw1i1esqbaXr2bR8o5/vOc9voT5zyxnxPDuOymfXrqKBUtWMmbk8I32r12fzHl0ERPHj25d4Q0sfG4VAGNHDe+l5cAZPizYbdstCRr+N9u0caNHMHH8aBr/Cvy5MSOHM36LEWw5agTDOn14y1EjGD1+GKNHDmP0iOGMHB4bvd8Xw4cFI4YFI4YHI4YNY8SwqPYN79jexENL0mZhl623HNDzt6LHqxlLgTHAYmBc/XojmTkTmAkwZcqUzep/wzOTZ5ev4feLVnDRd+bxqyeeY4ettuix/UMLl7Kho2Hi+NFsOWo4z61cyzZjR7HrhDEArFmXjBk1nGn79ZzSRw4bxq7b/Pl/UKNGDPvTcUobNizYbtzAhD5JkgabgQpec4CpwHXAZOCuAaqjX6xfnzz13Cq+ePvDfPcXT/D7RSv+9N7240fzsdcewt4TxzU8xm7bbsmIYcMYPszuBkmS2lXLg1dEHA8ckJmXdtr9ZeB7EXEMcABwd6vr6E/z/rCYi296gHXrk+dWrCZnz2bRmOexYpfdOPn5O/PKyTtx4E7PY2QPw4KSJGloalnwysxp9Z8/Bn7c5b1HI2I6Va/XhzJzXavq6A+LV6zh/143l98tXMaDTy1l2Pp1vHzJQ7zr2fvY45ZZMGZLtvjUx+Fl0we6VEmSNIgN1FAjmfkH4JqBOn+zVq5Zx+QLZwHw0RkHc9zXP8f2n/wYMWFrePGL4ctfhJe8BGckS5Kk3gxY8NpcnPO1nwNw74ems/WWo2DsG+Cb18KyZfDYY/DmN8OSJXDggXDQQTB5Mhx9NBxyCAwfvHfySZKk8pyE1MBzK9cw6/4n+ZdXHlCFLqgC1rx58O53w69+Bf/xH/Doo3DxxXDYYTB3Lpx6KkyYACecABdeCD/8ISz9sxs3JUnSEGOPVwPH/+dPATjl0J03fmP4cDj33GqI8dRT4Z574L/+C445pqPN00/DHXfA7bfDBRfAvffCfvtVvWFTp1Z/7tzluJIkqa0ZvHqwbn2y8LlVfPecqR29XV294AUwZ07VA9bVttvCK19ZfQGsWlW1ve02uOoqeOc7Yfz4KoAdfXQV2g480LlikiS1MYNXD1asqW603HdSL4+RHDsWDj+89wOOHg1HHVV9AWTCAw9UPWK3314NWa5fD696VfU1dSqM8PJIktRO/Je9B08vrR5107K1uCJg//2rr7e8pQpi8+bBDTfAP/5jNW/sFa+oQtj06bDlwD7iQJIk/eWcXN+NR/64jBf9x83sud3YcieNqO6KPP98mD27GpY87DD45Cdhxx3hlFPgK1+p5o5JkqTNksGrG5+79SG2GzeaH//DiwauiN12g7PPhh/9CB5+GGbMqHrD9toLjj++CmTz5w9cfZIkqc8MXt24++FneN2UXYjBMtF9m23gtNPgG9+AJ56A9763ukvyBS+ovj75SVi0aKCrlCRJvTB4dWPF6nWccOAOA11G97bcEk46Cb7whSqEXXwx3H037LknnH56dddk5kBXKUmSumHw6sbY0cMZM3IzWHV+xIjqsUVXXQUPPgiHHgpve1u1LMXHP+58MEmSBhmDV7vYbjv4+7+H+++Hz362mpy/997VAq8//am9YJIkDQIGr3YTUS3GesUV8NBD1Rpj73xntWzFf/4nLFw40BVKkjRkGby6sWDxyoEuoX9ssw285z3wy19Wc8Luuw/22Qfe8Ab4yU/sBZMkqTCDVxfLVq1lycq1bD9+9ECX0n8iqscSffnL1dIUU6fCu95V3RF59dWwdu1AVyhJ0pBg8OpiZf2ooAlje3g+4+ZuwgR497urXrALL4RLL4V994VPfxqWLx/o6iRJamsGry7m/WEJ40YPgScpDRtWPcD7ttuq+WCzZlVLUlx0kXdDSpLUIgavLuY/s5wDdnzeQJdR1tFHwze/CTffXD0j8kMfGuiKJElqS0Oga6dv/rh0Fdu06zBjb/76r+Hyywe6CkmS2pY9Xl0Ewb6Txg10GZIkqQ0ZvCRJkgoxeG2GvvQlOOOM1h3/5pth2rS+f+6CC6qvnkybVh27kRtugN12q0Y9f/az3s/5/e/DiSdW9wncc0+1793vhh126PgaMwa+8pWe20uSVIpzvDYDzz5bha33vnegK2mtp56Ct761esLRFlvA618Ps2f33P6hh6r1Ya+6qronYMYMeOyxaoWMSy+t2qxZA4ccAiec0HP7iDLfnyRJ9nhtBp59tnrmdbvo2iO1ww5w2mlVb9T06dUzvvfeu+r5+s1vej7Ok0/Cpz5VrQM7YwasXAlLl27c5oor4KUvrc7RTHtJklrJ4DXI/e3fwgtfWPXM7LBDFSKg6sk59VTYdlt49as7nv4zbRpcdx286lVw/PEdx/ne96rHNU6atPFw4Gc/C7vuCttvD+efv/G5/+EfqmdvH3ssrFhR7bvyymq5rz32qHrhenPRRbDjjtXw3pIl1b5LL4UFCzb+uuIK+P3vq96pDXbfHR58sOdjH3lkFdTWrOkIVOPHb9zmk5+Ec85pvr0kSa1k8BrkvvpV+N//rcLRggVw003V/q9/vXrk4qOPwu23w733dnzmn/8ZzjwTrr++er1wIZx9drVG6oMPwrXXws9/Xr33T/8EN94I8+fDAw/Ac89V+++8swpYTzxRBaabboJf/xrOOw9uuQXuuKNa7usXv+i59nvuqR4Ref/9VQCbO7fx97puHTyv0xJqY8dWvX29OffcKiSeffbG+++8swp9e+7ZXHtJklrN4LWZmjKlmiA+bhzstx8sXtzx3plnwkknwVZbVa/vugsefxwOP7x6RvYf/gDz5lXvTZ0KH/gAfO1r8JnPdPQATZpUPc5x5EiYPLk6/g9+AK94RRUCd9oJTjmlGh7syR13wMtfXj2l6IUvhIMPrvb3NNQ4YcLGQWvFimqB/d5ccgnMmVN93xuCI1S9aG98Y/PtJUlqNYNXF8tXryUHuogm7L13x3bXyeFHHLHx60w47riOYb3f/74angT41reqobgHHoCDDqp6x6DqJdpw3M7H77rdaGJ65sbvbwhRPQ01TplS9VJtMHs27LJLz8e/91743e+q7YMPrsLg/PnV6/Xr4dvfroJiM+0lSSrB4NXFzFsfYrDd5LbtttXjE5cvr75WrOjbnXhHHFENLT7wAKxeXc1zmjWrOtZBB8Fhh1VDgePGdcyp6u7406fDd75T9Z498UQ1lHnCCT2f9/DDq2HMxYurpSHuu69xnYcfDr/9LXz+8zBzZlXL4Yf33P6++6q7IFeurHrwFiyoevSgClmTJsE22zTXXpKkElxOootM+Lsjdh/oMjYyfnw1L2nvvauenI98pG+f3377KsycdFI1tPbGN8LJJ1fvvfOdVe/PmjXwspdVQefWW7s/zv77V+eeOrX6OV14YcfwYXeOPrqah7bvvtVw6AEHNK4zonpk5DnnVPPKvv51GDWqurHg5S//8+B22mnV/Lfdd4eJE+Hqq6v2UC1J0bXnr1F7SZJKiMzBP7A2ZcqUnN1oQad+dPAF3+e2c49nqzEji5xPkiS1l4iYk5lTunvPoUZJkqRCDF6SJEmFGLw6+dn8RTy3ci2jhvtjkSRJ/c+E0cmDTy5l8q5bM2bU8IEuRZIktSGDVxf7bj9uoEuQJEltyuAlSZJUiMGrk4VLV7F+8K+uIUmSNlMGr05+t3ApI4YNtnXrJUlSu3Dl+k5GDR/G5F23HugyJElSm7LHq5P5zywf6BIkSVIbM3jV1q1P7vjd02wz1of3SZKk1jB41VauWQfA8ftvP8CVSJKkdmXwqq2vHxY+0lXrJUlSi5gyanMfWzzQJUiSpDZn8KolydS/2m6gy5AkSW3M4CVJklSIwUuSJKkQg5ckSVIhBq/akhVrWb12/UCXIUmS2pjBq/brBUsYPdIfhyRJah2TRm3EsGEc6nMaJUlSCxm8JEmSCjF4SZIkFWLwkiRJKsTgJUmSVIjBS5IkqRCDVy3JgS5BkiS1OYNX7cGnlrLDVmMGugxJktTGDF6151auZcettxjoMiRJUhszeEmSJBVi8JIkSSrE4CVJklSIwUuSJKkQg5ckSVIhBi9JkqRCDF6SJEmFGLwkSZIKMXhJkiQVYvCSJEkqpCXBKyIuj4g7I+L8Ht6fEBHfi4jZEfHZVtQgSZI02PR78IqIGcDwzDwS2Csi9umm2WnAVZk5BRgfEVP6uw5JkqTBphU9XtOAa+rtWcDUbto8DRwUEVsDuwKPtaAOSZKkQaUVwWss8Hi9/QwwqZs2twG7A+cAv6rbbSQizqqHImcvXLiwBWVKkiSV1YrgtRQYU2+P6+Ec/wK8PTMvAn4NvLlrg8ycmZlTMnPKxIkTW1CmJElSWa0IXnPoGF6cDDzSTZsJwMERMRz4GyBbUIckSdKg0orgdQNwWkRcArwOmBcRH+7S5iPATGAxsA3wtRbUIUmSNKiM6O8DZuaSiJgGTAcuzswFwNwube4BDuzvc0uSJA1m/R68ADJzER13NkqSJAlXrpckSSrG4CVJklSIwUuSJKkQg5ckSVIhBi9JkqRCDF6SJEmFGLwkSZIKMXhJkiQVYvCSJEkqxOBVW59JDHQRkiSprRm8aqvWrGeLkcMHugxJktTGDF615WvWsuUog5ckSWodg1dt+ep1Bi9JktRSBq/aitXrGDNqxECXIUmS2pjBq7Z89Tq2dI6XJElqIYNXbX0mw4Z5X6MkSWodg5ckSVIhBi9JkqRCDF6SJEmFGLwkSZIKMXhJkiQVYvCSJEkqxOAlSZJUiMFLkiSpEIOXJElSIQYvSZKkQgxekiRJhRi8JEmSCjF4SZIkFWLwkiRJKsTgJUmSVIjBS5IkqRCDlyRJUiEGL0mSpEIMXpIkSYUYvCRJkgoxeEmSJBVi8JIkSSrE4CVJklSIwUuSJKkQg5ckSVIhBi9JkqRCDF6SJEmFGLwkSZIKMXhJkiQVYvCSJEkqxOAlSZJUiMFLkiSpEIOXJElSIQYvSZKkQgxekiRJhRi8JEmSCjF4SZIkFWLwkiRJKsTgJUmSVIjBS5IkqRCDlyRJUiEGL0mSpEIMXpIkSYUYvCRJkgoxeEmSJBVi8JIkSSrE4CVJklSIwUuSJKkQg5ckSVIhBi9JkqRCDF6SJEmFGLwkSZIKMXhJkiQVMqKZRhExAdgJeAZ4MjPXt7QqSZKkNtRrj1dEnAvcCHwNOB74UhOfuTwi7oyI83tpd1lEvLLJWiVJkjZrzQw1vjIzjwCezsyrgL0aNY6IGcDwzDwS2Csi9umh3THADpn57b4WLUmStDlqJngtiYjTgS0i4kXAs720nwZcU2/PAqZ2bRARI4HPAY9ExMnNlytJkrT5aiZ4nQEcCiwCTgbe0kv7scDj9fYzwKRu2pwO3A9cDBweEWd3bRARZ0XE7IiYvXDhwibKlCRJGtx6DV6Z+VRm/n1mviwz30cVrBpZCoypt8f1cI5DgZmZuQC4Ejium/POzMwpmTll4sSJvZUpSZI06DUzuf6KLruu7OUjc+gYXpwMPNJNmwfpmCs2BXi0tzokSZI2dz0uJxERuwF7AgdGxLH17rHAml6OeQNwa0TsBJwIvCEiPpyZne9wvBz4QkS8ARgJvGZTvwFJkqTNRaN1vPakmig/of4zgBXAmY0OmJlLImIaMB24uB5OnNulzXPAaze1aEmSpM1Rj8ErM38K/DQids/Mi/py0MxcRMedjZIkSaK5yfUb9XBFxI6tK0eSJKl99frIoIj4V+AkqjsUAZYBh7SyKEmSpHbUzDpexwJHAfdQBS4X1ZIkSdoEzQSvYVTLQoyjCl4uqiVJkrQJmglerwNWAx8E3gH8a0srkiRJalM9Bq+IGB4R/wfYPzNnZ+a9VI8PylLFSZIktZNGk+u/SjWRflxEnAL8Dngr8CPgugK1SZIktZVGwWvXzDwqIgJ4GLgMOCYzny1TmiRJUntpFLy2iIgjqVasfwa4DTggIsjMO4pUJ0mS1EYaBa+5wFmdtt9Wbydg8JIkSeqjRo8MenPJQiRJktpdM8tJSJIkqR8YvCRJkgoxeEmSJBVi8JIkSSqk0V2NfxIRBwE7A/OBxzJzaUurkiRJakO99nhFxKeAC4GPAHtRrWgvSZKkPmpmqPHgzHw18GxmfhfYqsU1SZIktaVmgtfCiPgQMCEi3gQsaHFNkiRJbamZ4HU6sBi4k6q3y4VVJUmSNkEzk+tfBszMzBWtLkaSJKmdNRO89gG+HhGLgG8B38nMZa0tS5Ikqf30OtSYmR/NzJcBbwf2BR5teVWSJEltqNcer4g4CTgR2AW4Bzim1UVJkiS1o2aGGg8CLsnM37a6GEmSpHbWa/DKzH8vUYgkSVK781mNkiRJhfTY4xURl2Tm+yLiJ0Bu2A1kZh5fpDpJkqQ20mPwysz31X8eV64cSZKk9uVQoyRJUiF9Dl4RMbUVhUiSJLW7XoNXRPygy66PtKgWSZKkttZocv0hwKHAzhFxer17LLCyRGGSJEntplGPV3Tz59PA61pakSRJUptqdFfjXGBuROyXmV8pWJMkSVJbauYh2f9cohBJkqR253ISkiRJhbhyvSRJUiGuXC9JklSIQ42SJEmFNLOA6rCIeF5EjIiI4yJifInCJEmS2k0zPV7XAscC/wW8Fbi+pRVJkiS1qWaC17aZ+R1gn8w8FRjT4pokSZLaUjPB67mIuAGYExEvA55rcU2SJEltqce7Gjt5LXBAZv4sIiYDr29xTZIkSW2pmR6vtcCUiPgv4IXAstaWJEmS1J6aCV5fBHYEbgJ2rl9LkiSpj5oZatwlM0+rt78fETe3sB5JkqS21UzweiIi3g/cDRwB/KG1JUmSJLWnZoYazwCWAK8Gnq1fS5IkqY8aPSR7B+AcYDnwicx0GQlJkqS/QKMeryuAeVS9XJeVKUeSJKl9NZrjNSozrwKIiNcUqkeSJKltNQpeEyPib4EAtq+3AcjMr7a8MkmSpDbTKHj9D7BPN9vZ0ookSZLaVI/BKzMvLFmIJElSu2tmOQlJkiT1A4OXJElSIQYvSZKkQgxekiRJhTTzrEYi4iBgZ2A+8FhmLm1pVZIkSW2o1x6viPgUcCHwEWAvwDW8JEmSNkEzQ40HZ+argWcz87vAVi2uSZIkqS01E7wWRsSHgAkR8SZgQYtrkiRJakvNBK/TgcXAnVS9XWe0siBJkqR21Uzwei2wCLgbeLZ+LUmSpD5qJnhF/TUGmAEc29KKJEmS2lSvy0lk5pc7vfzviLishfVIkiS1rV6DV0R07uGaCBzQunIkSZLaVzMLqB7XaXs18K4W1SJJktTWmhlqvLCvB42Iy6l6xr6bmR9u0G4ScFNmHtrXc0iSJG1umlm5/sa+HDAiZgDDM/NIYK+I2KdB849RTdqXJElqe83c1fiLiDi5D8ecBlxTb88CpnbXKCKOB5bhgqySJGmIaCZ4vRC4OiLuiYifRMSPe2k/Fni83n4GmNS1QUSMAj4InNfTQSLirIiYHRGzFy5c2ESZkiRJg1szc7yO661NF0vpGD4cR/fh7jzgssx8NiJ6Ou9MYCbAlClTso81SJIkDTo99nj1cXixszl0DC9OBh7pps1LgHdFxM3A8yPi85t4LkmSpM1Go6HG92ziMW8ATouIS4DXAfMiYqM7GzPz2MyclpnTgHsz862beC5JkqTNRqOhxiMi4jdd9gWQmblvTx/KzCURMQ2YDlycmQuAuQ3aT2u+XEmSpM1Xo+B19ybM7wIgMxfRcWejJEmSaDzUeF2xKiRJkoaAHoNXZn66ZCGSJEntrpl1vCRJktQPDF6SJEmFGLwkSZIKMXhJkiQVYvCSJEkqxOAlSZJUiMFLkiSpEIOXJElSIQYvSZKkQgxekiRJhRi8JEmSCjF4SZIkFWLwkiRJKsTgJUmSVIjBS5IkqRCDlyRJUiEGL0mSpEIMXpIkSYUYvCRJkgoxeEmSJBVi8JIkSSrE4CVJklSIwUuSJKkQg5ckSVIhBi9JkqRCDF6SJEmFGLwkSZIKMXhJkiQVYvCSJEkqxOAlSZJUiMFLkiSpEIOXJElSIQYvSZKkQgxekiRJhRi8JEmSCjF4SZIkFWLwkiRJKsTgJUmSVIjBS5IkqRCDlyRJUiEGL0mSpEIMXpIkSYUYvCRJkgoxeEmSJBVi8JIkSSrE4CVJklSIwUuSJKkQg5ckSVIhBi9JkqRCDF6SJEmFGLwkSZIKMXhJkiQVYvCSJEkqxOAlSZJUiMFLkiSpEIOXJElSIQYvSZKkQgxekiRJhRi8JEmSCjF4SZIkFWLwkiRJKsTgJUmSVIjBS5IkqRCDlyRJUiEGL0mSpEIMXpIkSYUYvCRJkgoxeEmSJBXSkuAVEZdHxJ0RcX4P728VETdGxKyIuD4iRrWiDkmSpMGk34NXRMwAhmfmkcBeEbFPN7pvKekAAApcSURBVM1OBS7JzBOABcBL+7sOSZKkwWZEC445Dbim3p4FTAV+27lBZl7W6eVE4KkW1CFJkjSotGKocSzweL39DDCpp4YRcSQwITPv6ua9syJidkTMXrhwYQvKlCRJKqsVwWspMKbeHtfTOSJiG+BTwJndvZ+ZMzNzSmZOmThxYgvKlCRJKqsVwWsO1fAiwGTgka4N6sn01wLvz8xHW1CDJEnSoNOK4HUDcFpEXAK8DpgXER/u0uYtwGHAByLi5oh4fQvqkCRJGlT6fXJ9Zi6JiGnAdODizFwAzO3S5jPAZ/r73JIkSYNZK+5qJDMX0XFnoyRJknDlekmSpGIMXpIkSYUYvCRJkgoxeEmSJBVi8JIkSSrE4CVJklSIwUuSJKkQg5ckSVIhBi9JkqRCDF6SJEmFGLwkSZIKMXhJkiQVYvCSJEkqxOAlSZJUiMFLkiSpEIOXJElSIQYvSZKkQgxekiRJhRi8JEmSCjF4SZIkFWLwkiRJKsTgJUmSVIjBS5IkqRCDlyRJUiEGL0mSpEIMXpIkSYUYvCRJkgoxeEmSJBVi8JIkSSrE4CVJklSIwUuSJKkQg5ckSVIhBi9JkqRCDF6SJEmFGLwkSZIKMXhJkiQVYvCSJEkqxOAlSZJUiMFLkiSpEIOXJElSIQYvSZKkQgxekiRJhRi8JEmSCjF4SZIkFWLwkiRJKsTgJUmSVIjBS5IkqRCDlyRJUiEGL0mSpEIMXpIkSYUYvCRJkgoxeEmSJBVi8JIkSSrE4CVJklSIwUuSJKkQg5ckSVIhBi9JkqRCDF6SJEmFGLwkSZIKMXhJkiQVYvCSJEkqxOAlSZJUiMFLkiSpEIOXJElSIQYvSZKkQgxekiRJhRi8JEmSCjF4SZIkFWLwkiRJKsTgJUmSVEhLgldEXB4Rd0bE+X9JG0mSpHbS78ErImYAwzPzSGCviNhnU9pIkiS1m1b0eE0Drqm3ZwFTN7GNJElSW2lF8BoLPF5vPwNM2pQ2EXFWRMyOiNkLFy5sQZkbO/PoPRk9wilvkiSpdVqRNJYCY+rtcT2co9c2mTkzM6dk5pSJEye2oMyN/f30fdli5PCWn0eSJA1drQhec+gYOpwMPLKJbSRJktrKiBYc8wbg1ojYCTgReENEfDgzz2/Q5ogW1CFJkjSo9HuPV2YuoZo8fxdwXGbO7RK6umuzuL/rkCRJGmxa0eNFZi6i467FTW4jSZLUTryNT5IkqRCDlyRJUiEGL0mSpEIMXpIkSYUYvCRJkgoxeEmSJBVi8JIkSSrE4CVJklSIwUuSJKkQg5ckSVIhBi9JkqRCDF6SJEmFGLwkSZIKMXhJkiQVEpk50DX0KiIWAo8WONV2wB8LnEfN85oMPl6TwcnrMvh4TQanEtdl98yc2N0bm0XwKiUiZmfmlIGuQx28JoOP12Rw8roMPl6TwWmgr4tDjZIkSYUYvCRJkgoxeG1s5kAXoD/jNRl8vCaDk9dl8PGaDE4Del2c4yVJklSIPV6SJEmFGLwkSZIKGXLBKyIuj4g7I+L8v6SN+ldvP/OI2CoiboyIWRFxfUSMKl3jUNPs70FETIqIn5eqa6jrw3W5LCJeWaquoayJv78mRMT3ImJ2RHy2dH1DVf13060N3h8ZEd+OiNsj4sxSdQ2p4BURM4DhmXkksFdE7LMpbdS/mvyZnwpckpknAAuAl5ascajp4+/Bx4AxZSob2pq9LhFxDLBDZn67aIFDUJPX5DTgqnrtqPER4dpeLRYRE4AvA2MbNDsbmJOZRwOviYjxJWobUsELmAZcU2/PAqZuYhv1r2n08jPPzMsy8wf1y4nAU2VKG7Km0cTvQUQcDyyjCsNqvWn0cl0iYiTwOeCRiDi5XGlD1jR6/115GjgoIrYGdgUeK1PakLYOeD2wpEGbaXRcu1uAIoF4qAWvscDj9fYzwKRNbKP+1fTPPCKOBCZk5l0lChvCer0m9XDvB4HzCtY11DXzu3I6cD9wMXB4RJxdqLahqplrchuwO3AO8Ku6nVooM5dk5uJemg3Iv/dDLXgtpWNIZBzdf//NtFH/aupnHhHbAJ8Cio3FD2HNXJPzgMsy89liVamZ63IoMDMzFwBXAscVqm2oauaa/Avw9sy8CPg18OZCtamxAfn3fqiFijl0dANPBh7ZxDbqX73+zOvelWuB92dmiQemD3XN/B68BHhXRNwMPD8iPl+mtCGtmevyILBXvT0F8PeltZq5JhOAgyNiOPA3gAtoDg4D8u/9kFpANSKeB9wK/Ag4EXgD8NrMPL9BmyOa6K7UX6DJ6/IO4N+BufWuz2Tm/5Sudaho5pp0aX9zZk4rV+HQ1OTvynjgC1TDJiOB12Tm490cTv2gyWtyOPBFquHGO4FTMnPpAJQ75Gz4u6mej3pAZl7a6b3dge8BPwSOovr3fl3LaxpKwQv+dKfDdOCWuit+k9qof/kzH3y8JoOT12Xw8ZpsviJiJ6per++X6mQZcsFLkiRpoAy1OV6SJEkDxuAlSZJUiMFL0l8kIi6IiF9FxM3117t7aX9zP5/3loj4UT1Xo6/H+HiX18+PiOf31m5TRcSXIuLn9eNlrq0XO+2p7bSI2KM/zitp8DB4SeoP/5aZ0+qvS3tv3q/nPZbqjrE+LxSame/tsuv59Vdv7f4SZ9ePl1lKtSRHT6YBe/TjeSUNAgYvSf0uIsZFxE0RcWtEfLFBuzER8Z261+r6iBgREVtGxHX1vk83ecoJwIqIGB0RX4uIn0bEVRExqrtzdDr/zZ22P0K1KOx5EfGjLnV2bveBiHhVvf3+iHhtX2uOiKBasHF1ROwUEbfVP6t/q9//InAG8PGIuKreNymqB8XfERHvb/LnImmQMXhJ6g8fqIcZL6tf70j1lIGXAHtERE+P4jgAWN+p12occBbwy3rfjhFxSC/nvQU4AvgE8Lb6sy8Cfkv1lIPuzvFnMvP9wEeBj2bmixuc81qq9ZoAjqVaB6gvNX+KaqHGJ4EfAztTBb4TgVfWtbwZ+BLw3sw8tf7c+4H/ycyjgFdFxLYNziFpkBrRexNJ6tW/ZeaVnV6vAd5K9WiUbeh4LEdXPwN+GRGzqILSTcB+wFERMQ3YmiqY3NfMeSPiAOAb9cu7qMLMZ7s5xybLzN9ExC71wpnPZuayiOhLzWdTrRu0KjMzItZSPVJmKTC+wan3A46MiDOonjG3E9XDlyVtRuzxktQKbwGuA94ILGvQbjJwe2aeQDVceAzwAPDxeiX884H5fTjvPKreL+o/5/Vwjp6sALaEPw0H9uQe4L3At+rXfa35s8Bb6kfIvA/4CFVQ7bywYtdaHgDOq8/xUXzQsrRZMnhJaoUfUA2N/bh+vXMP7R4BzomIO4AdgNnA54AT6yHEtwOP9eG8nwcOrD+7D9VwXXfnaFT3jIi4ncYB7Vqq4PWd+nWfas7MRVQ/m1fXx/hvqhC3PCI2/Ky+TjXf7C5gb6qw9Y91bS+lGqqUtJlx5XpJkqRC7PGSJEkqxOAlSZJUiMFLkiSpEIOXJElSIQYvSZKkQgxekiRJhRi8JEmSCvn/fSbR+SuJAZwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.plot(fpr, tpr, lw=1)    # 画出当前分割数据的ROC曲线\n",
    "\n",
    "plt.xlim([-0.05, 1.05])     # 设置x、y轴的上下限，设置宽一点，以免和边缘重合，可以更好的观察图像的整体\n",
    "plt.ylim([-0.05, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')    # 可以使用中文，但需要导入一些库即字体\n",
    "plt.title('in_out-ROC-AUS: {:.4f}'.format(roc_auc))\n",
    "\n",
    "\n",
    "plt.annotate(r'threshold={:.3f}'.format(optimal_threshold), xy=(fpr[optimal_idx], tpr[optimal_idx]), xycoords='data', xytext=(+30, -30),\n",
    "             textcoords='offset points', fontsize=12, color='blue',\n",
    "             arrowprops=dict(arrowstyle='->', connectionstyle=\"arc3,rad=.1\",color='red'))\n",
    "plt.savefig(\"roc2.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

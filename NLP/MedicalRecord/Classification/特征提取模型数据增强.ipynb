{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89125401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8313 lines loaded!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import copy\n",
    "data = []\n",
    "with open(r'data\\data_model_2049.txt') as f:\n",
    "    for line in f.readlines()[1:]:\n",
    "        arr = line.strip().split(',')\n",
    "        arr_1 = arr[1].split('。')\n",
    "        for s in arr_1:\n",
    "            if s != '':\n",
    "                t_arr = copy.deepcopy(arr)\n",
    "                t_arr[1] = s + '。'\n",
    "                data.append(t_arr)\n",
    "print('%s lines loaded!' % len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80de1a33",
   "metadata": {},
   "source": [
    "### 放射痛（右肩、肩胛和背部）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "171aa781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "放射腹部平片DR：腹部部分肠腔积气\n",
      "我院门诊2021-02-28放射腹部平片DR：考虑不完全肠梗阻\n",
      "放射胸部正侧位DR未见异常\n",
      "2021-04-08放射腹部平片DR腹部部分肠管积气\n",
      "行放射性钡灌肠插管检查示1.乙状结肠憩室2.阑尾部分显影\n",
      "0 88\n",
      "1 48\n",
      "2 5\n",
      "3 23880\n"
     ]
    }
   ],
   "source": [
    "sents = {'0': [], '1': [], '2': [], '3': []}\n",
    "for line in data:\n",
    "    text, flag = line[1], line[2]\n",
    "    start, end = 1000, -1\n",
    "    it = re.finditer('放射', text)\n",
    "    for match in it:\n",
    "        if match.span(0)[0] < start:\n",
    "            start = match.span(0)[0]\n",
    "        if match.span(0)[1] > end:\n",
    "            end = match.span(0)[1]\n",
    "    span = text[start:end] if end != -1 else ''\n",
    "    if span != '':\n",
    "        span = span.replace('(', '\\(').replace(')', '\\)')\n",
    "        match = re.search('[^，。,；]*' + span + '[^，。,；]*', text)\n",
    "#         if match is None:\n",
    "#             print(text)\n",
    "#             print(span)\n",
    "        s = match.group(0)\n",
    "        sents[flag].append(s)\n",
    "        if flag == '2':\n",
    "            print(s)\n",
    "    arr = re.split('[，,；;]', text.replace(s, ''))\n",
    "    for r in arr:\n",
    "        if r != '':\n",
    "            sents['3'].append(r + '，')\n",
    "\n",
    "for key in sents.keys():\n",
    "    sents[key] = list(set(sents[key]))\n",
    "    random.shuffle(sents[key])\n",
    "    print(key, len(sents[key]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7d6937d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "result = []\n",
    "for i in range(1000):\n",
    "    for key in ['0', '1', '2']:\n",
    "        s_arr = random.sample(sents['3'], random.randint(3, 10))\n",
    "        if key == '2':\n",
    "            if random.randint(0, 10) == 0:\n",
    "                s = random.choice(sents[key])\n",
    "            else:\n",
    "                s = ''\n",
    "        else:\n",
    "            s = random.choice(sents[key])\n",
    "        s_arr.append(s)\n",
    "        random.shuffle(s_arr)\n",
    "        result.append(('，'.join(s_arr), key))\n",
    "\n",
    "print(len(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "956ca1e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 4, 1]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.sample([0,1,2,3,4,5,6,7,8], 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6c213535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 222\n",
      "1 74\n",
      "2 144\n",
      "3 7107\n"
     ]
    }
   ],
   "source": [
    "sents = {'0': [], '1': [], '2': [], '3': []}\n",
    "for line in data:\n",
    "    text, flag = line[1], line[2]\n",
    "    start, end = 1000, -1\n",
    "    match = re.search('放射|肩|背', text)\n",
    "    if match:\n",
    "        sents[flag].append(text)\n",
    "    else:\n",
    "        sents['3'].append(text)\n",
    "\n",
    "for key in sents.keys():\n",
    "    sents[key] = list(set(sents[key]))\n",
    "    print(key, len(sents[key]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "75576375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "result = []\n",
    "for i in range(1000):\n",
    "    for key in ['0', '1', '2']:\n",
    "        if key == '2':\n",
    "            if random.randint(0, 3) == 0:\n",
    "                s = random.choice(sents[key])\n",
    "            else:\n",
    "                s = random.choice(sents['3'])\n",
    "        else:\n",
    "            s = random.choice(sents[key])\n",
    "        s2 = random.choice(sents['3'])\n",
    "        arr = [s, s2]\n",
    "        result.append((''.join(arr), key))\n",
    "\n",
    "print(len(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0f1237bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'data\\data_model_gen_4.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write('xx,文本,放射痛（右肩、肩胛和背部）\\n')\n",
    "    for text, flag in result:\n",
    "        f.write('xx,%s,%s\\n' % (text, flag))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "782926d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:root:Loading data lines...\n",
      "DEBUG:root:3001 Lines Loaded!\n",
      "DEBUG:root:Training Feature: 放射痛（右肩、肩胛和背部）\n",
      "DEBUG:root:Statistic Class Sample Number...\n",
      "DEBUG:root:Class Sample Number: {'0': 1000, '1': 1000, '2': 1000}\n",
      "INFO:root:Initializing Class TextClassifier...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 total num: 1000, train num: 800\n",
      "1 total num: 1000, train num: 800\n",
      "2 total num: 1000, train num: 800\n",
      "train data size: 2400, val data size: 600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../../BertModels/medical-roberta-wwm were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at ../../BertModels/medical-roberta-wwm and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "DEBUG:root:Initializing Model BertTextClassfication\n",
      "INFO:root:TextClassifier Using Device: CUDA\n",
      "INFO:root:TextClassifier Loading Data...\n",
      "DEBUG:root:Using Text Pair? -> Yes\n",
      "DEBUG:root:Start Model Traning....\n",
      "DEBUG:root:Epoch: 0/100, Training Loss:1.148539, Acc:0.352917, elapsed: 118.084609\n",
      "DEBUG:root:Evalaton loss:1.132807, Acc:0.333333, elapsed: 15.551032\n",
      "DEBUG:root:Saving Model: output/models/textclassify/放射痛（右肩、肩胛和背部）_20220729145650_3333.pth\n",
      "DEBUG:root:Epoch: 1/100, Training Loss:1.139287, Acc:0.318333, elapsed: 116.597555\n",
      "DEBUG:root:Evalaton loss:1.105569, Acc:0.333333, elapsed: 15.585679\n",
      "DEBUG:root:Epoch: 2/100, Training Loss:1.124060, Acc:0.328750, elapsed: 117.112000\n",
      "DEBUG:root:Evalaton loss:1.111814, Acc:0.333333, elapsed: 15.423198\n",
      "DEBUG:root:Epoch: 3/100, Training Loss:1.116491, Acc:0.317917, elapsed: 117.098835\n",
      "DEBUG:root:Evalaton loss:1.117041, Acc:0.333333, elapsed: 15.461137\n",
      "DEBUG:root:Epoch: 4/100, Training Loss:1.112846, Acc:0.343750, elapsed: 117.209929\n",
      "DEBUG:root:Evalaton loss:1.099812, Acc:0.333333, elapsed: 15.567048\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrun\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_all_features\n\u001b[1;32m----> 2\u001b[0m \u001b[43mtrain_all_features\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mdata_model_gen.txt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_field\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_fields\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moutput/textclassify_train_20220729.txt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\projects\\NLP\\MedicalRecord\\Classification\\run.py:50\u001b[0m, in \u001b[0;36mtrain_all_features\u001b[1;34m(file_path, start_field, num_fields, log_file)\u001b[0m\n\u001b[0;32m     44\u001b[0m model \u001b[38;5;241m=\u001b[39m TextClassifier(model_save_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput/models/textclassify\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     45\u001b[0m                         pre_model_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../../BertModels/medical-roberta-wwm\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     46\u001b[0m                         num_cls\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,\n\u001b[0;32m     47\u001b[0m                         model_name\u001b[38;5;241m=\u001b[39mfeature_name)\n\u001b[0;32m     49\u001b[0m model\u001b[38;5;241m.\u001b[39mload_train_val_data(train_data, val_data, label_dict\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m2\u001b[39m}, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m)\n\u001b[1;32m---> 50\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwrite_result_to_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stopping_num\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\projects\\NLP\\MedicalRecord\\Classification\\Lib\\TextClassifier.py:193\u001b[0m, in \u001b[0;36mTextClassifier.train\u001b[1;34m(self, epochs, early_stopping_num, write_result_to_file)\u001b[0m\n\u001b[0;32m    191\u001b[0m \u001b[38;5;66;03m#计算误差\u001b[39;00m\n\u001b[0;32m    192\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_func(out, labels)\n\u001b[1;32m--> 193\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    194\u001b[0m \u001b[38;5;66;03m#误差反向传播\u001b[39;00m\n\u001b[0;32m    195\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from run import train_all_features\n",
    "train_all_features(r'data\\data_model_gen.txt', start_field=2, num_fields=3, log_file=r'output/textclassify_train_20220729.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3901da38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

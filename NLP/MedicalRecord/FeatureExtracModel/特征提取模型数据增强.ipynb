{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5b69ee4",
   "metadata": {},
   "source": [
    "### 将标记数据文本按照。拆分成多条标记数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89125401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11921 lines loaded!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import copy\n",
    "result_labeled_data, text_field = [], 2\n",
    "with open(r'data\\data_xbs_mdlfeature.txt') as f:\n",
    "    for line in f.readlines()[1:]:\n",
    "        line_arr = line.strip().split('\t')\n",
    "        text_arr = line_arr[text_field].split('。')\n",
    "        for s in text_arr:\n",
    "            if s != '':\n",
    "                t_arr = copy.deepcopy(line_arr)\n",
    "                t_arr[text_field] = s + '。'\n",
    "                result_labeled_data.append(t_arr)\n",
    "print('%s lines loaded!' % len(result_labeled_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b223e66e",
   "metadata": {},
   "source": [
    "### 使用正则来将文本拆分为短句，并赋予类别标签"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad6d0460",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def classify_text_snippet(labeled_data, label_field, regex, print_flag='2'):\n",
    "    \"\"\"\n",
    "    使用正则来拆分文本句子，将包含正则的短句划分到相应的标记类别，没有正则的短句划分到相应的标记类别\n",
    "    \"\"\"\n",
    "    sents = {'0': [], '1': [], '2': [], '3': []}\n",
    "    for line in labeled_data:\n",
    "        text, flag = line[text_field], line[label_field]\n",
    "        \n",
    "        # 正则匹配上\n",
    "        start, end = 1000, -1\n",
    "        it = re.finditer(regex, text)\n",
    "        for match in it:\n",
    "            if match.span(0)[0] < start:\n",
    "                start = match.span(0)[0]\n",
    "            if match.span(0)[1] > end:\n",
    "                end = match.span(0)[1]\n",
    "        span = text[start:end] if end != -1 else ''\n",
    "        if span != '':\n",
    "            span = span.replace('(', '\\(').replace(')', '\\)').replace('^', '\\^').replace('+', '\\+').replace('?', '\\?').replace('.', '\\.')\n",
    "            span = span.replace('*', '\\*')\n",
    "            match = re.search('[^，。,；]*' + span + '[^，。,；]*', text)\n",
    "#             if match is None:\n",
    "#                 print(text)\n",
    "#                 print(span)\n",
    "            s = match.group(0)\n",
    "            sents[flag].append(s)\n",
    "            text = text.replace(s, '')\n",
    "            if flag == print_flag:\n",
    "                print(line[0], s)\n",
    "        \n",
    "        arr = re.split('[，,；;]', text)\n",
    "        for r in arr:\n",
    "            if r != '':\n",
    "                sents['3'].append(r + '，')\n",
    "\n",
    "    for key in sents.keys():\n",
    "        sents[key] = list(set(sents[key]))\n",
    "        random.shuffle(sents[key])\n",
    "        print(key, len(sents[key]))\n",
    "        \n",
    "    return sents\n",
    "\n",
    "def generate_snippet_to_text(sents_dict):\n",
    "    \"\"\"\n",
    "    将短句合并生成长文本\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    for i in range(1000):\n",
    "        for key in ['0', '1', '2']:\n",
    "            s_arr = random.sample(sents_dict['3'], random.randint(3, 10))\n",
    "            if key == '2':\n",
    "                if random.randint(0, 10) == 0:\n",
    "                    s = random.choice(sents_dict[key])\n",
    "                else:\n",
    "                    s = ''\n",
    "            else:\n",
    "                s = random.choice(sents_dict[key])\n",
    "            s_arr.append(s)\n",
    "            random.shuffle(s_arr)\n",
    "            result.append(('，'.join(s_arr), key))\n",
    "\n",
    "    print(len(result))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ef3b30",
   "metadata": {},
   "source": [
    "### 使用正则来筛选整句"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cee74ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classif_text(labeled_data, label_field, regex):\n",
    "    sents = {'0': [], '1': [], '2': [], '3': []}\n",
    "    for line in labeled_data:\n",
    "        text, flag = line[text_field], line[label_field]\n",
    "        start, end = 1000, -1\n",
    "        match = re.search(regex, text)\n",
    "        if match:\n",
    "            sents[flag].append(text)\n",
    "        else:\n",
    "            sents['3'].append(text)\n",
    "\n",
    "    for key in sents.keys():\n",
    "        sents[key] = list(set(sents[key]))\n",
    "        print(key, len(sents[key]))\n",
    "        \n",
    "    return sents\n",
    "\n",
    "def generate_text(sents_dict):\n",
    "    result = []\n",
    "    for i in range(1000):\n",
    "        for key in ['0', '1', '2']:\n",
    "            if key == '2':\n",
    "                if random.randint(0, 3) == 0:\n",
    "                    s = random.choice(sents_dict[key])\n",
    "                else:\n",
    "                    s = random.choice(sents_dict['3'])\n",
    "            else:\n",
    "                s = random.choice(sents_dict[key])\n",
    "            s2 = random.choice(sents_dict['3'])\n",
    "            arr = [s, s2]\n",
    "            result.append((''.join(arr), key))\n",
    "\n",
    "    print(len(result))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5eaa186",
   "metadata": {},
   "outputs": [],
   "source": [
    "from run import train1, predict\n",
    "\n",
    "\n",
    "def write_data_to_file(data, file_name, field_name):\n",
    "    with open(r'data\\%s.txt' % file_name, 'w', encoding='utf-8') as f:\n",
    "        f.write('文本,%s\\n' % field_name)\n",
    "        for text, flag in data:\n",
    "            f.write('%s,%s\\n' % (text, flag))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80de1a33",
   "metadata": {},
   "source": [
    "### 放射痛（右肩、肩胛和背部）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "171aa781",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10002192 放射至后背\n",
      "10005033 有腰背部放射痛\n",
      "10010721 向背部放射\n",
      "10014098 伴后背放射痛\n",
      "10015132 伴后背放射痛\n",
      "10017235 伴腰背部放射\n",
      "10024192 无腰背部放射痛\n",
      "10030179 不向腰背部放射\n",
      "10032606 伴腰背部放射\n",
      "10033502 无腰背部放射痛\n",
      "10034091 伴后背放射痛\n",
      "10037144 疼痛向腰背部放射\n",
      "10037989 无腰背部放射痛\n",
      "10054640 向腰背部放射\n",
      "10055680 疼痛向后背放射\n",
      "10055694 向腰背部放射\n",
      "10068883 逐渐放射至右侧腰背部\n",
      "10073519 无左下腹放射痛\n",
      "10088057 向腰背部放射\n",
      "10093985 不伴胸痛、腰背放射痛\n",
      "10094425 不向其他部位放射\n",
      "10098446 放射腹部平片DR：腹部部分肠腔积气\n",
      "10105038 伴右背部放射性疼痛\n",
      "10112211 未向肩背部放射\n",
      "10121253 疼痛向背部放射\n",
      "10122455 我院门诊2021-02-28放射腹部平片DR：考虑不完全肠梗阻\n",
      "10126833 无腰背部放射痛\n",
      "10139381 无腰背部及会阴部放射痛\n",
      "10142410 无腰背部放射痛\n",
      "10142415 无腰背部放射痛\n",
      "10147487 无腰背部放射痛\n",
      "10149504 伴有背部放射痛\n",
      "10149753 伴有腰背部放射痛\n",
      "10150713 无腰背部放射痛\n",
      "10151531 伴后背放射痛\n",
      "10151729 无腰背部放射痛\n",
      "10159623 偶有后背放射痛\n",
      "10161009 伴腰背部及肩胛部放射痛\n",
      "10173503 放射胸部正侧位DR未见异常\n",
      "10174498 伴胸背部放射痛\n",
      "10174964 向腰背部放射\n",
      "10182612 向腰背部放射\n",
      "10185220 不向腰背部放射\n",
      "10188273 放射至背部\n",
      "10204932 完善头胸腹部CT提示：1.两侧基底节及放射冠区腔隙性脑梗塞\n",
      "20007984 不伴腰背部及胸部放射痛\n",
      "20011315 疼痛不向其他部位放射\n",
      "20016867 不伴腰背部放射痛\n",
      "20022489 患者自述3天前在无明显诱因下出现间断性上腹部胀痛伴有后背部放射性疼痛\n",
      "20028634 无腰背部放射性疼痛\n",
      "20031081 患者自述1天前在无明显诱因下出现上腹部胀痛伴有后背部放射性疼痛\n",
      "20043206 无腰背部放射痛\n",
      "20049211 无腰背部放射性疼痛\n",
      "20049633 不伴腰背部放射痛\n",
      "20050245 2021-07-05放射腹部平片DR考虑不全肠梗阻\n",
      "20051804 不伴腰背部放射痛\n",
      "20052925 伴有后背部放射性疼痛\n",
      "20053804 疼痛向腰背部放射\n",
      "0 117\n",
      "1 99\n",
      "2 36\n",
      "3 32679\n",
      "3000\n",
      "0 363\n",
      "1 213\n",
      "2 209\n",
      "3 9906\n",
      "3000\n"
     ]
    }
   ],
   "source": [
    "sents1 = classify_text_snippet(result_labeled_data, 3, '放射', '2')\n",
    "data1 = generate_snippet_to_text(sents1)\n",
    "sents2 = classif_text(result_labeled_data, 3, '放射|肩|背')\n",
    "data2 = generate_text(sents2)\n",
    "write_data_to_file(data1 + data2, '放射痛_右肩', '放射痛_右肩')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d6937d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:root:Loading data lines...\n",
      "DEBUG:root:6001 Lines Loaded!\n",
      "DEBUG:root:Statistic Class Sample Number...\n",
      "DEBUG:root:Class Sample Number: {'0': 2000, '1': 2000, '2': 2000}\n",
      "INFO:root:Initializing Class TextClassifier...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0, total num: 2000, train num: 1600\n",
      "1, total num: 2000, train num: 1600\n",
      "2, total num: 2000, train num: 1600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../../BertModels/medical-roberta-wwm were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at ../../BertModels/medical-roberta-wwm and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "DEBUG:root:Initializing Model BertTextClassfication\n",
      "INFO:root:TextClassifier Using Device: CUDA\n",
      "INFO:root:TextClassifier Loading Data...\n",
      "DEBUG:root:Using Text Pair? -> Yes\n",
      "DEBUG:root:Start Model Traning....\n",
      "DEBUG:root:Epoch: 0/100, Training Loss:0.511265, Acc:0.820625, elapsed: 227.750040\n",
      "DEBUG:root:Evalaton loss:0.320497, Acc:0.905833, elapsed: 23.917983\n",
      "DEBUG:root:Saving Model: output/models/放射痛_右肩_20220816084317_9058.pth\n",
      "DEBUG:root:Epoch: 1/100, Training Loss:0.406249, Acc:0.867292, elapsed: 225.919977\n",
      "DEBUG:root:Evalaton loss:0.345117, Acc:0.899167, elapsed: 24.006047\n",
      "DEBUG:root:Epoch: 2/100, Training Loss:0.398750, Acc:0.868542, elapsed: 226.189974\n",
      "DEBUG:root:Evalaton loss:0.383204, Acc:0.874167, elapsed: 23.918974\n",
      "DEBUG:root:Epoch: 3/100, Training Loss:0.347424, Acc:0.893958, elapsed: 224.704698\n",
      "DEBUG:root:Evalaton loss:0.369395, Acc:0.887500, elapsed: 24.306978\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43m放射痛_右肩.txt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_field\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_field\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moutput/logs/放射痛_右肩_train_20220812.txt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\projects\\NLP\\MedicalRecord\\FeatureExtracModel\\run.py:79\u001b[0m, in \u001b[0;36mtrain1\u001b[1;34m(file_path, text_field, label_field, log_file)\u001b[0m\n\u001b[0;32m     73\u001b[0m model \u001b[38;5;241m=\u001b[39m TextClassifier(model_save_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput/models\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     74\u001b[0m                         pre_model_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../../BertModels/medical-roberta-wwm\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     75\u001b[0m                         num_cls\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,\n\u001b[0;32m     76\u001b[0m                         model_name\u001b[38;5;241m=\u001b[39mfeature_name)\n\u001b[0;32m     78\u001b[0m model\u001b[38;5;241m.\u001b[39mload_train_val_data(train_data, val_data, label_dict\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m2\u001b[39m}, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m)\n\u001b[1;32m---> 79\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwrite_result_to_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stopping_num\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\projects\\NLP\\MedicalRecord\\FeatureExtracModel\\Lib\\TextClassifier.py:202\u001b[0m, in \u001b[0;36mTextClassifier.train\u001b[1;34m(self, epochs, early_stopping_num, write_result_to_file)\u001b[0m\n\u001b[0;32m    200\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    201\u001b[0m \u001b[38;5;66;03m#计算acc\u001b[39;00m\n\u001b[1;32m--> 202\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mout\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m    203\u001b[0m labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m    204\u001b[0m train_acc \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflat_accuracy(out, labels)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train1(file_path=r'data\\放射痛_右肩.txt', text_field=0, label_field=1, log_file=r'output/logs/放射痛_右肩_train_20220812.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85b3312f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:root:Loading data lines...\n",
      "DEBUG:root:689 Lines Loaded!\n",
      "INFO:root:Initializing Class TextClassifier...\n",
      "DEBUG:root:Loading Model output/models/放射痛_右肩_数据增强_20220816084317_9058.pth\n",
      "INFO:root:TextClassifier Using Device: CUDA\n",
      "INFO:root:TextClassifier Loading Data...\n",
      "DEBUG:root:Using Text Pair? -> Yes\n",
      "DEBUG:root:Data For Predicting\n",
      "DEBUG:root:Loading data lines...\n",
      "DEBUG:root:184 Lines Loaded!\n",
      "INFO:root:Initializing Class TextClassifier...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best accuracy: 0.8318965517241379\n",
      "{0: '0', 1: '1', 2: '2'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:root:Loading Model output/models/放射痛_右肩_数据增强_20220816084317_9058.pth\n",
      "INFO:root:TextClassifier Using Device: CUDA\n",
      "INFO:root:TextClassifier Loading Data...\n",
      "DEBUG:root:Using Text Pair? -> Yes\n",
      "DEBUG:root:Data For Predicting\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best accuracy: 0.967391304347826\n",
      "{0: '0', 1: '1', 2: '2'}\n"
     ]
    }
   ],
   "source": [
    "best_mdl_path = r'output/models/放射痛_右肩_数据增强_20220816084317_9058.pth'\n",
    "col, name = 3, '放射痛（右肩、肩胛和背部）'\n",
    "predict(best_mdl_path, predict_file_path=r'data\\测试集_现病史_模型.txt', result_file_path=r'output/data/%s_现病史_测试集结果.txt' % name, \n",
    "        log_file_path=r'output/data/predict_20220815_2.txt', num_fields=24, text_field=2, label_field=col, feature_name=name, skip_title=True)\n",
    "predict(best_mdl_path, predict_file_path=r'data\\人机_现病史_模型.txt', result_file_path=r'output/data/%s_现病史_人机结果.txt' % name, \n",
    "        log_file_path=r'output/data/predict_20220815_2.txt', num_fields=24, text_field=2, label_field=col, feature_name=name, skip_title=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64712796",
   "metadata": {},
   "source": [
    "### 厌食"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c213535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10172369 3天前食“鳝鱼”后疼痛较前加重\n",
      "10195259 未进食\n",
      "20014750 未进饮食\n",
      "20023741 未进食\n",
      "0 547\n",
      "1 887\n",
      "2 3\n",
      "3 31092\n",
      "3000\n",
      "0 1131\n",
      "1 1935\n",
      "2 4\n",
      "3 7642\n",
      "3000\n"
     ]
    }
   ],
   "source": [
    "sents1 = classify_text_snippet(result_labeled_data, 5, '食', '2')\n",
    "data1 = generate_snippet_to_text(sents1)\n",
    "sents2 = classif_text(result_labeled_data, 5, '食')\n",
    "data2 = generate_text(sents2)\n",
    "write_data_to_file(data1 + data2, '厌食', '厌食')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75576375",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:root:Loading data lines...\n",
      "DEBUG:root:6001 Lines Loaded!\n",
      "DEBUG:root:Statistic Class Sample Number...\n",
      "DEBUG:root:Class Sample Number: {'2': 2000, '0': 2000, '1': 2000}\n",
      "INFO:root:Initializing Class TextClassifier...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2, total num: 2000, train num: 1600\n",
      "0, total num: 2000, train num: 1600\n",
      "1, total num: 2000, train num: 1600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../../BertModels/medical-roberta-wwm were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at ../../BertModels/medical-roberta-wwm and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "DEBUG:root:Initializing Model BertTextClassfication\n",
      "INFO:root:TextClassifier Using Device: CUDA\n",
      "INFO:root:TextClassifier Loading Data...\n",
      "DEBUG:root:Using Text Pair? -> Yes\n",
      "DEBUG:root:Start Model Traning....\n",
      "DEBUG:root:Epoch: 0/100, Training Loss:0.896150, Acc:0.524375, elapsed: 228.390716\n",
      "DEBUG:root:Evalaton loss:0.695063, Acc:0.604167, elapsed: 24.189626\n",
      "DEBUG:root:Saving Model: output/models/厌食_20220815114259_6041.pth\n",
      "DEBUG:root:Epoch: 1/100, Training Loss:0.702827, Acc:0.605833, elapsed: 226.181486\n",
      "DEBUG:root:Evalaton loss:0.707704, Acc:0.602500, elapsed: 23.916409\n",
      "DEBUG:root:Epoch: 2/100, Training Loss:0.625616, Acc:0.632292, elapsed: 226.086747\n",
      "DEBUG:root:Evalaton loss:0.565795, Acc:0.636667, elapsed: 24.039281\n",
      "DEBUG:root:Saving Model: output/models/厌食_20220815115120_6366.pth\n",
      "DEBUG:root:Epoch: 3/100, Training Loss:0.571941, Acc:0.635417, elapsed: 226.096589\n",
      "DEBUG:root:Evalaton loss:0.572754, Acc:0.640833, elapsed: 23.856057\n",
      "DEBUG:root:Saving Model: output/models/厌食_20220815115531_6408.pth\n",
      "DEBUG:root:Epoch: 4/100, Training Loss:0.565510, Acc:0.638542, elapsed: 226.422869\n",
      "DEBUG:root:Evalaton loss:0.564493, Acc:0.643333, elapsed: 23.970746\n",
      "DEBUG:root:Saving Model: output/models/厌食_20220815115942_6433.pth\n",
      "DEBUG:root:Epoch: 5/100, Training Loss:0.568102, Acc:0.642708, elapsed: 226.211179\n",
      "DEBUG:root:Evalaton loss:0.556273, Acc:0.646667, elapsed: 23.951195\n",
      "DEBUG:root:Saving Model: output/models/厌食_20220815120353_6466.pth\n",
      "DEBUG:root:Epoch: 6/100, Training Loss:0.549723, Acc:0.649167, elapsed: 225.982170\n",
      "DEBUG:root:Evalaton loss:0.553237, Acc:0.646667, elapsed: 23.861104\n",
      "DEBUG:root:Epoch: 7/100, Training Loss:0.547024, Acc:0.644583, elapsed: 226.416821\n",
      "DEBUG:root:Evalaton loss:0.554092, Acc:0.646667, elapsed: 24.125259\n",
      "DEBUG:root:Epoch: 8/100, Training Loss:0.539452, Acc:0.657500, elapsed: 226.408772\n",
      "DEBUG:root:Evalaton loss:0.537338, Acc:0.648333, elapsed: 23.891035\n",
      "DEBUG:root:Saving Model: output/models/厌食_20220815121624_6483.pth\n",
      "DEBUG:root:Epoch: 9/100, Training Loss:0.537300, Acc:0.644792, elapsed: 226.843435\n",
      "DEBUG:root:Evalaton loss:0.557011, Acc:0.649167, elapsed: 23.918893\n",
      "DEBUG:root:Saving Model: output/models/厌食_20220815122036_6491.pth\n",
      "DEBUG:root:Epoch: 10/100, Training Loss:0.531242, Acc:0.655000, elapsed: 226.510681\n",
      "DEBUG:root:Evalaton loss:0.533337, Acc:0.647500, elapsed: 24.024578\n",
      "DEBUG:root:Epoch: 11/100, Training Loss:0.525488, Acc:0.651250, elapsed: 226.104468\n",
      "DEBUG:root:Evalaton loss:0.550925, Acc:0.645000, elapsed: 24.024971\n",
      "DEBUG:root:Epoch: 12/100, Training Loss:0.527039, Acc:0.648542, elapsed: 225.918947\n",
      "DEBUG:root:Evalaton loss:0.547319, Acc:0.649167, elapsed: 23.987042\n",
      "DEBUG:root:Epoch: 13/100, Training Loss:0.517036, Acc:0.649167, elapsed: 226.138988\n",
      "DEBUG:root:Evalaton loss:0.543051, Acc:0.645833, elapsed: 24.001392\n",
      "DEBUG:root:Epoch: 14/100, Training Loss:0.508920, Acc:0.653750, elapsed: 226.116456\n",
      "DEBUG:root:Evalaton loss:0.573650, Acc:0.658333, elapsed: 23.978187\n",
      "DEBUG:root:Saving Model: output/models/厌食_20220815124127_6583.pth\n",
      "DEBUG:root:Epoch: 15/100, Training Loss:0.509286, Acc:0.650000, elapsed: 225.970264\n",
      "DEBUG:root:Evalaton loss:0.550967, Acc:0.646667, elapsed: 24.078697\n",
      "DEBUG:root:Epoch: 16/100, Training Loss:0.507101, Acc:0.662917, elapsed: 226.375234\n",
      "DEBUG:root:Evalaton loss:0.542430, Acc:0.650000, elapsed: 23.918827\n",
      "DEBUG:root:Epoch: 17/100, Training Loss:0.498604, Acc:0.672500, elapsed: 226.096175\n",
      "DEBUG:root:Evalaton loss:0.562231, Acc:0.646667, elapsed: 24.015074\n",
      "DEBUG:root:Epoch: 18/100, Training Loss:0.498564, Acc:0.659167, elapsed: 226.113001\n",
      "DEBUG:root:Evalaton loss:0.551769, Acc:0.649167, elapsed: 24.002851\n",
      "DEBUG:root:Epoch: 19/100, Training Loss:0.497288, Acc:0.660833, elapsed: 226.122133\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrun\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train1\n\u001b[1;32m----> 2\u001b[0m best_mdl_path \u001b[38;5;241m=\u001b[39m \u001b[43mtrain1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43m厌食.txt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_field\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_field\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moutput/logs/厌食_train_20220811.txt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(best_mdl_path)\n",
      "File \u001b[1;32mD:\\projects\\NLP\\MedicalRecord\\FeatureExtracModel\\run.py:74\u001b[0m, in \u001b[0;36mtrain1\u001b[1;34m(file_path, text_field, label_field, log_file)\u001b[0m\n\u001b[0;32m     68\u001b[0m model \u001b[38;5;241m=\u001b[39m TextClassifier(model_save_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput/models\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     69\u001b[0m                         pre_model_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../../BertModels/medical-roberta-wwm\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     70\u001b[0m                         num_cls\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,\n\u001b[0;32m     71\u001b[0m                         model_name\u001b[38;5;241m=\u001b[39mfeature_name)\n\u001b[0;32m     73\u001b[0m model\u001b[38;5;241m.\u001b[39mload_train_val_data(train_data, val_data, label_dict\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m2\u001b[39m}, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m)\n\u001b[1;32m---> 74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwrite_result_to_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stopping_num\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\projects\\NLP\\MedicalRecord\\FeatureExtracModel\\Lib\\TextClassifier.py:222\u001b[0m, in \u001b[0;36mTextClassifier.train\u001b[1;34m(self, epochs, early_stopping_num, write_result_to_file)\u001b[0m\n\u001b[0;32m    220\u001b[0m val_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss_func(out, labels)\n\u001b[0;32m    221\u001b[0m \u001b[38;5;66;03m#计算acc\u001b[39;00m\n\u001b[1;32m--> 222\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mout\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m    223\u001b[0m labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m    224\u001b[0m val_acc \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflat_accuracy(out, labels)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from run import train1\n",
    "best_mdl_path = train1(file_path=r'data\\厌食.txt', text_field=0, label_field=1, log_file=r'output/logs/厌食_train_20220811.txt')\n",
    "print(best_mdl_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0f1237bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3901da38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
